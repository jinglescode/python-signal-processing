{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from torch.nn.functional import elu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_to_th(\n",
    "    X, requires_grad=False, dtype=None, pin_memory=False, **tensor_kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Convenience function to transform numpy array to `torch.Tensor`.\n",
    "    Converts `X` to ndarray using asarray if necessary.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: ndarray or list or number\n",
    "        Input arrays\n",
    "    requires_grad: bool\n",
    "        passed on to Variable constructor\n",
    "    dtype: numpy dtype, optional\n",
    "    var_kwargs:\n",
    "        passed on to Variable constructor\n",
    "    Returns\n",
    "    -------\n",
    "    var: `torch.Tensor`\n",
    "    \"\"\"\n",
    "    if not hasattr(X, \"__len__\"):\n",
    "        X = [X]\n",
    "    X = np.asarray(X)\n",
    "    if dtype is not None:\n",
    "        X = X.astype(dtype)\n",
    "    X_tensor = torch.tensor(X, requires_grad=requires_grad, **tensor_kwargs)\n",
    "    if pin_memory:\n",
    "        X_tensor = X_tensor.pin_memory()\n",
    "    return X_tensor\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def transpose_time_to_spat(x):\n",
    "    \"\"\"Swap time and spatial dimensions.\n",
    "    Returns\n",
    "    -------\n",
    "    x: torch.Tensor\n",
    "        tensor in which last and first dimensions are swapped\n",
    "    \"\"\"\n",
    "    return x.permute(0, 3, 2, 1)\n",
    "\n",
    "def squeeze_final_output(x):\n",
    "    \"\"\"Removes empty dimension at end and potentially removes empty time\n",
    "     dimension. It does  not just use squeeze as we never want to remove\n",
    "     first dimension.\n",
    "    Returns\n",
    "    -------\n",
    "    x: torch.Tensor\n",
    "        squeezed tensor\n",
    "    \"\"\"\n",
    "\n",
    "    assert x.size()[3] == 1\n",
    "    x = x[:, :, :, 0]\n",
    "    if x.size()[2] == 1:\n",
    "        x = x[:, :, 0]\n",
    "    return x\n",
    "\n",
    "\n",
    "class Expression(nn.Module):\n",
    "    \"\"\"Compute given expression on forward pass.\n",
    "    Parameters\n",
    "    ----------\n",
    "    expression_fn : callable\n",
    "        Should accept variable number of objects of type\n",
    "        `torch.autograd.Variable` to compute its output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, expression_fn):\n",
    "        super(Expression, self).__init__()\n",
    "        self.expression_fn = expression_fn\n",
    "\n",
    "    def forward(self, *x):\n",
    "        return self.expression_fn(*x)\n",
    "\n",
    "    def __repr__(self):\n",
    "        if hasattr(self.expression_fn, \"func\") and hasattr(\n",
    "            self.expression_fn, \"kwargs\"\n",
    "        ):\n",
    "            expression_str = \"{:s} {:s}\".format(\n",
    "                self.expression_fn.func.__name__, str(self.expression_fn.kwargs)\n",
    "            )\n",
    "        elif hasattr(self.expression_fn, \"__name__\"):\n",
    "            expression_str = self.expression_fn.__name__\n",
    "        else:\n",
    "            expression_str = repr(self.expression_fn)\n",
    "        return (\n",
    "            self.__class__.__name__ +\n",
    "            \"(expression=%s) \" % expression_str\n",
    "        )\n",
    "\n",
    "\n",
    "class AvgPool2dWithConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute average pooling using a convolution, to have the dilation parameter.\n",
    "    Parameters\n",
    "    ----------\n",
    "    kernel_size: (int,int)\n",
    "        Size of the pooling region.\n",
    "    stride: (int,int)\n",
    "        Stride of the pooling operation.\n",
    "    dilation: int or (int,int)\n",
    "        Dilation applied to the pooling filter.\n",
    "    padding: int or (int,int)\n",
    "        Padding applied before the pooling operation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size, stride, dilation=1, padding=0):\n",
    "        super(AvgPool2dWithConv, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "        self.padding = padding\n",
    "        # don't name them \"weights\" to\n",
    "        # make sure these are not accidentally used by some procedure\n",
    "        # that initializes parameters or something\n",
    "        self._pool_weights = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Create weights for the convolution on demand:\n",
    "        # size or type of x changed...\n",
    "        in_channels = x.size()[1]\n",
    "        weight_shape = (\n",
    "            in_channels,\n",
    "            1,\n",
    "            self.kernel_size[0],\n",
    "            self.kernel_size[1],\n",
    "        )\n",
    "        if self._pool_weights is None or (\n",
    "            (tuple(self._pool_weights.size()) != tuple(weight_shape)) or\n",
    "            (self._pool_weights.is_cuda != x.is_cuda) or\n",
    "            (self._pool_weights.data.type() != x.data.type())\n",
    "        ):\n",
    "            n_pool = np.prod(self.kernel_size)\n",
    "            weights = np_to_th(\n",
    "                np.ones(weight_shape, dtype=np.float32) / float(n_pool)\n",
    "            )\n",
    "            weights = weights.type_as(x)\n",
    "            if x.is_cuda:\n",
    "                weights = weights.cuda()\n",
    "            self._pool_weights = weights\n",
    "\n",
    "        pooled = F.conv2d(\n",
    "            x,\n",
    "            self._pool_weights,\n",
    "            bias=None,\n",
    "            stride=self.stride,\n",
    "            dilation=self.dilation,\n",
    "            padding=self.padding,\n",
    "            groups=in_channels,\n",
    "        )\n",
    "        return pooled\n",
    "    \n",
    "class Ensure4d(nn.Module):\n",
    "    def forward(self, x):\n",
    "        while(len(x.shape) < 4):\n",
    "            x = x.unsqueeze(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep4Net(nn.Sequential):\n",
    "    \"\"\"Deep ConvNet model from Schirrmeister et al 2017.\n",
    "    Model described in [Schirrmeister2017]_.\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_chans : int\n",
    "        XXX\n",
    "    References\n",
    "    ----------\n",
    "    .. [Schirrmeister2017] Schirrmeister, R. T., Springenberg, J. T., Fiederer,\n",
    "       L. D. J., Glasstetter, M., Eggensperger, K., Tangermann, M., Hutter, F.\n",
    "       & Ball, T. (2017).\n",
    "       Deep learning with convolutional neural networks for EEG decoding and\n",
    "       visualization.\n",
    "       Human Brain Mapping , Aug. 2017.\n",
    "       Online: http://dx.doi.org/10.1002/hbm.23730\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chans,\n",
    "        n_classes,\n",
    "        input_window_samples,\n",
    "        final_conv_length=\"auto\",\n",
    "        n_filters_time=25,\n",
    "        n_filters_spat=25,\n",
    "        filter_time_length=10,\n",
    "        pool_time_length=1,\n",
    "        pool_time_stride=1,\n",
    "        n_filters_2=50,\n",
    "        filter_length_2=10,\n",
    "        n_filters_3=100,\n",
    "        filter_length_3=10,\n",
    "        n_filters_4=200,\n",
    "        filter_length_4=10,\n",
    "        first_nonlin=elu,\n",
    "        first_pool_mode=\"max\",\n",
    "        first_pool_nonlin=identity,\n",
    "        later_nonlin=elu,\n",
    "        later_pool_mode=\"max\",\n",
    "        later_pool_nonlin=identity,\n",
    "        drop_prob=0.5,\n",
    "        double_time_convs=False,\n",
    "        split_first_layer=True,\n",
    "        batch_norm=True,\n",
    "        batch_norm_alpha=0.1,\n",
    "        stride_before_pool=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if final_conv_length == \"auto\":\n",
    "            assert input_window_samples is not None\n",
    "        self.in_chans = in_chans\n",
    "        self.n_classes = n_classes\n",
    "        self.input_window_samples = input_window_samples\n",
    "        self.final_conv_length = final_conv_length\n",
    "        self.n_filters_time = n_filters_time\n",
    "        self.n_filters_spat = n_filters_spat\n",
    "        self.filter_time_length = filter_time_length\n",
    "        self.pool_time_length = pool_time_length\n",
    "        self.pool_time_stride = pool_time_stride\n",
    "        self.n_filters_2 = n_filters_2\n",
    "        self.filter_length_2 = filter_length_2\n",
    "        self.n_filters_3 = n_filters_3\n",
    "        self.filter_length_3 = filter_length_3\n",
    "        self.n_filters_4 = n_filters_4\n",
    "        self.filter_length_4 = filter_length_4\n",
    "        self.first_nonlin = first_nonlin\n",
    "        self.first_pool_mode = first_pool_mode\n",
    "        self.first_pool_nonlin = first_pool_nonlin\n",
    "        self.later_nonlin = later_nonlin\n",
    "        self.later_pool_mode = later_pool_mode\n",
    "        self.later_pool_nonlin = later_pool_nonlin\n",
    "        self.drop_prob = drop_prob\n",
    "        self.double_time_convs = double_time_convs\n",
    "        self.split_first_layer = split_first_layer\n",
    "        self.batch_norm = batch_norm\n",
    "        self.batch_norm_alpha = batch_norm_alpha\n",
    "        self.stride_before_pool = stride_before_pool\n",
    "\n",
    "        if self.stride_before_pool:\n",
    "            conv_stride = self.pool_time_stride\n",
    "            pool_stride = 1\n",
    "        else:\n",
    "            conv_stride = 1\n",
    "            pool_stride = self.pool_time_stride\n",
    "        self.add_module(\"ensuredims\", Ensure4d())\n",
    "        pool_class_dict = dict(max=nn.MaxPool2d, mean=AvgPool2dWithConv)\n",
    "        first_pool_class = pool_class_dict[self.first_pool_mode]\n",
    "        later_pool_class = pool_class_dict[self.later_pool_mode]\n",
    "        if self.split_first_layer:\n",
    "            self.add_module(\"dimshuffle\", Expression(transpose_time_to_spat))\n",
    "            self.add_module(\n",
    "                \"conv_time\",\n",
    "                nn.Conv2d(\n",
    "                    1,\n",
    "                    self.n_filters_time,\n",
    "                    (self.filter_time_length, 1),\n",
    "                    stride=1,\n",
    "                ),\n",
    "            )\n",
    "            self.add_module(\n",
    "                \"conv_spat\",\n",
    "                nn.Conv2d(\n",
    "                    self.n_filters_time,\n",
    "                    self.n_filters_spat,\n",
    "                    (1, self.in_chans),\n",
    "                    stride=(conv_stride, 1),\n",
    "                    bias=not self.batch_norm,\n",
    "                ),\n",
    "            )\n",
    "            n_filters_conv = self.n_filters_spat\n",
    "        else:\n",
    "            self.add_module(\n",
    "                \"conv_time\",\n",
    "                nn.Conv2d(\n",
    "                    self.in_chans,\n",
    "                    self.n_filters_time,\n",
    "                    (self.filter_time_length, 1),\n",
    "                    stride=(conv_stride, 1),\n",
    "                    bias=not self.batch_norm,\n",
    "                ),\n",
    "            )\n",
    "            n_filters_conv = self.n_filters_time\n",
    "        if self.batch_norm:\n",
    "            self.add_module(\n",
    "                \"bnorm\",\n",
    "                nn.BatchNorm2d(\n",
    "                    n_filters_conv,\n",
    "                    momentum=self.batch_norm_alpha,\n",
    "                    affine=True,\n",
    "                    eps=1e-5,\n",
    "                ),\n",
    "            )\n",
    "        self.add_module(\"conv_nonlin\", Expression(self.first_nonlin))\n",
    "        self.add_module(\n",
    "            \"pool\",\n",
    "            first_pool_class(\n",
    "                kernel_size=(self.pool_time_length, 1), stride=(pool_stride, 1)\n",
    "            ),\n",
    "        )\n",
    "        self.add_module(\"pool_nonlin\", Expression(self.first_pool_nonlin))\n",
    "\n",
    "        def add_conv_pool_block(\n",
    "            model, n_filters_before, n_filters, filter_length, block_nr\n",
    "        ):\n",
    "            suffix = \"_{:d}\".format(block_nr)\n",
    "            self.add_module(\"drop\" + suffix, nn.Dropout(p=self.drop_prob))\n",
    "            self.add_module(\n",
    "                \"conv\" + suffix,\n",
    "                nn.Conv2d(\n",
    "                    n_filters_before,\n",
    "                    n_filters,\n",
    "                    (filter_length, 1),\n",
    "                    stride=(conv_stride, 1),\n",
    "                    bias=not self.batch_norm,\n",
    "                ),\n",
    "            )\n",
    "            if self.batch_norm:\n",
    "                self.add_module(\n",
    "                    \"bnorm\" + suffix,\n",
    "                    nn.BatchNorm2d(\n",
    "                        n_filters,\n",
    "                        momentum=self.batch_norm_alpha,\n",
    "                        affine=True,\n",
    "                        eps=1e-5,\n",
    "                    ),\n",
    "                )\n",
    "            self.add_module(\"nonlin\" + suffix, Expression(self.later_nonlin))\n",
    "\n",
    "            self.add_module(\n",
    "                \"pool\" + suffix,\n",
    "                later_pool_class(\n",
    "                    kernel_size=(self.pool_time_length, 1),\n",
    "                    stride=(pool_stride, 1),\n",
    "                ),\n",
    "            )\n",
    "            self.add_module(\n",
    "                \"pool_nonlin\" + suffix, Expression(self.later_pool_nonlin)\n",
    "            )\n",
    "\n",
    "        add_conv_pool_block(\n",
    "            self, n_filters_conv, self.n_filters_2, self.filter_length_2, 2\n",
    "        )\n",
    "        add_conv_pool_block(\n",
    "            self, self.n_filters_2, self.n_filters_3, self.filter_length_3, 3\n",
    "        )\n",
    "        add_conv_pool_block(\n",
    "            self, self.n_filters_3, self.n_filters_4, self.filter_length_4, 4\n",
    "        )\n",
    "\n",
    "        # self.add_module('drop_classifier', nn.Dropout(p=self.drop_prob))\n",
    "        self.eval()\n",
    "        if self.final_conv_length == \"auto\":\n",
    "            out = self(\n",
    "                np_to_th(\n",
    "                    np.ones(\n",
    "                        (1, self.in_chans, self.input_window_samples, 1),\n",
    "                        dtype=np.float32,\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            n_out_time = out.cpu().data.numpy().shape[2]\n",
    "            self.final_conv_length = n_out_time\n",
    "        self.add_module(\n",
    "            \"conv_classifier\",\n",
    "            nn.Conv2d(\n",
    "                self.n_filters_4,\n",
    "                self.final_conv_length,\n",
    "                (self.final_conv_length, 1),\n",
    "                bias=True,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.add_module(\"softmax\", nn.LogSoftmax(dim=1))\n",
    "        self.add_module(\"squeeze\", Expression(squeeze_final_output))\n",
    "\n",
    "        # Initialization, xavier is same as in our paper...\n",
    "        # was default from lasagne\n",
    "        init.xavier_uniform_(self.conv_time.weight, gain=1)\n",
    "        # maybe no bias in case of no split layer and batch norm\n",
    "        if self.split_first_layer or (not self.batch_norm):\n",
    "            init.constant_(self.conv_time.bias, 0)\n",
    "        if self.split_first_layer:\n",
    "            init.xavier_uniform_(self.conv_spat.weight, gain=1)\n",
    "            if not self.batch_norm:\n",
    "                init.constant_(self.conv_spat.bias, 0)\n",
    "        if self.batch_norm:\n",
    "            init.constant_(self.bnorm.weight, 1)\n",
    "            init.constant_(self.bnorm.bias, 0)\n",
    "        param_dict = dict(list(self.named_parameters()))\n",
    "        for block_nr in range(2, 5):\n",
    "            conv_weight = param_dict[\"conv_{:d}.weight\".format(block_nr)]\n",
    "            init.xavier_uniform_(conv_weight, gain=1)\n",
    "            if not self.batch_norm:\n",
    "                conv_bias = param_dict[\"conv_{:d}.bias\".format(block_nr)]\n",
    "                init.constant_(conv_bias, 0)\n",
    "            else:\n",
    "                bnorm_weight = param_dict[\"bnorm_{:d}.weight\".format(block_nr)]\n",
    "                bnorm_bias = param_dict[\"bnorm_{:d}.bias\".format(block_nr)]\n",
    "                init.constant_(bnorm_weight, 1)\n",
    "                init.constant_(bnorm_bias, 0)\n",
    "\n",
    "        init.xavier_uniform_(self.conv_classifier.weight, gain=1)\n",
    "        init.constant_(self.conv_classifier.bias, 0)\n",
    "\n",
    "        # Start in eval mode\n",
    "        self.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 214])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = Deep4Net(\n",
    "#     in_chans=10,\n",
    "#     n_classes=11,\n",
    "#     input_window_samples=250,\n",
    "#     final_conv_length=\"auto\"\n",
    "# )\n",
    "# # model\n",
    "# x = torch.rand(3, 10, 250)\n",
    "# y = model(x)\n",
    "# y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214\n",
      "torch.Size([3, 200, 214, 1])\n",
      "2 torch.Size([3, 214, 214])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 214, 214])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_backbone_and_fc(backbone):\n",
    "    \n",
    "    classifier = nn.Sequential()\n",
    "    classifier.add_module(\n",
    "        \"conv_classifier\",\n",
    "        backbone.conv_classifier\n",
    "    )\n",
    "    classifier.add_module(\"softmax\", backbone.softmax)\n",
    "    classifier.add_module(\"squeeze\", backbone.squeeze)\n",
    "\n",
    "    backbone.conv_classifier = torch.nn.Identity()\n",
    "    backbone.softmax = torch.nn.Identity()\n",
    "    backbone.squeeze = torch.nn.Identity()\n",
    "    return backbone, classifier\n",
    "\n",
    "class Deep4NetModel(nn.Module):\n",
    "    def __init__(self, num_channel=10, num_classes=4, signal_length=1000):\n",
    "        super().__init__()\n",
    "        \n",
    "        base_model = Deep4Net(\n",
    "            in_chans=num_channel,\n",
    "            n_classes=num_classes,\n",
    "            input_window_samples=signal_length,\n",
    "            final_conv_length=\"auto\"\n",
    "        )\n",
    "        \n",
    "        self.backbone, self.fc = get_backbone_and_fc(base_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        print(x.shape)\n",
    "        x = self.fc(x)\n",
    "        print(2, x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Deep4NetModel(\n",
    "    num_channel=10,\n",
    "    num_classes=11,\n",
    "    signal_length=250,\n",
    ")\n",
    "\n",
    "x = torch.rand(3, 10, 250)\n",
    "y = model(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from deep4net import Deep4NetModel\n",
    "# import torch\n",
    "# # model = Deep4Net(\n",
    "# #     in_chans=10,\n",
    "# #     n_classes=11,\n",
    "# #     input_window_samples=250,\n",
    "# #     final_conv_length=\"auto\"\n",
    "# # )\n",
    "\n",
    "# model = Deep4NetModel(\n",
    "#     num_channel=10,\n",
    "#     num_classes=11,\n",
    "#     signal_length=250,\n",
    "# )\n",
    "# # model\n",
    "# x = torch.rand(3, 10, 250)\n",
    "# y = model(x)\n",
    "# y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1234"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "import sys\n",
    "path = os.path.join(cwd, \"..\\\\..\\\\\")\n",
    "sys.path.append(path)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from torch.nn.functional import elu\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import logging\n",
    "logging.getLogger('lightning').setLevel(0)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pytorch_lightning\n",
    "pytorch_lightning.utilities.distributed.log.setLevel(logging.ERROR)\n",
    "\n",
    "from splearn.data import MultipleSubjects, PyTorchDataset, PyTorchDataset2Views, HSSSVEP\n",
    "from splearn.filter.butterworth import butter_bandpass_filter\n",
    "from splearn.filter.notch import notch_filter\n",
    "from splearn.filter.channels import pick_channels\n",
    "from splearn.nn.models import CompactEEGNet\n",
    "from splearn.utils import Logger, Config\n",
    "from splearn.nn.base import LightningModelClassifier\n",
    "\n",
    "####\n",
    "\n",
    "config = {\n",
    "    \"run_name\": \"deep4net_normal\",\n",
    "    \"data\": {\n",
    "        \"load_subject_ids\": np.arange(1,36),\n",
    "        # \"selected_channels\": [\"PO8\", \"PZ\", \"PO7\", \"PO4\", \"POz\", \"PO3\", \"O2\", \"Oz\", \"O1\"], # AA paper\n",
    "        \"selected_channels\": [\"PZ\", \"PO5\", \"PO3\", \"POz\", \"PO4\", \"PO6\", \"O1\", \"Oz\", \"O2\"], # hsssvep paper\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"num_epochs\": 500,\n",
    "        \"num_warmup_epochs\": 50,\n",
    "        \"learning_rate\": 0.03,\n",
    "        \"gpus\": [0],\n",
    "        \"batchsize\": 256,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"optimizer\": \"adamw\",\n",
    "        \"scheduler\": \"cosine_with_warmup\",\n",
    "    },\n",
    "    \"testing\": {\n",
    "        \"test_subject_ids\": np.arange(1,36),\n",
    "        \"kfolds\": np.arange(0,3),\n",
    "    },\n",
    "    \"seed\": 1234\n",
    "}\n",
    "\n",
    "main_logger = Logger(filename_postfix=config[\"run_name\"])\n",
    "main_logger.write_to_log(\"Config\")\n",
    "main_logger.write_to_log(config)\n",
    "\n",
    "config = Config(config)\n",
    "\n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load subject: 1\n",
      "Load subject: 2\n",
      "Load subject: 3\n",
      "Load subject: 4\n",
      "Load subject: 5\n",
      "Load subject: 6\n",
      "Load subject: 7\n",
      "Load subject: 8\n",
      "Load subject: 9\n",
      "Load subject: 10\n",
      "Load subject: 11\n",
      "Load subject: 12\n",
      "Load subject: 13\n",
      "Load subject: 14\n",
      "Load subject: 15\n",
      "Load subject: 16\n",
      "Load subject: 17\n",
      "Load subject: 18\n",
      "Load subject: 19\n",
      "Load subject: 20\n",
      "Load subject: 21\n",
      "Load subject: 22\n",
      "Load subject: 23\n",
      "Load subject: 24\n",
      "Load subject: 25\n",
      "Load subject: 26\n",
      "Load subject: 27\n",
      "Load subject: 28\n",
      "Load subject: 29\n",
      "Load subject: 30\n",
      "Load subject: 31\n",
      "Load subject: 32\n",
      "Load subject: 33\n",
      "Load subject: 34\n",
      "Load subject: 35\n",
      "Final data shape: (35, 240, 9, 250) (35, 240)\n"
     ]
    }
   ],
   "source": [
    "def func_preprocessing(data):\n",
    "    data_x = data.data\n",
    "    data_x = pick_channels(data_x, channel_names=data.channel_names, selected_channels=config.data.selected_channels)\n",
    "    # data_x = notch_filter(data_x, sampling_rate=data.sampling_rate, notch_freq=50.0)\n",
    "    data_x = butter_bandpass_filter(data_x, lowcut=7, highcut=90, sampling_rate=data.sampling_rate, order=6)\n",
    "    start_t = 160\n",
    "    end_t = start_t + 250\n",
    "    data_x = data_x[:,:,:,start_t:end_t]\n",
    "    data.set_data(data_x)\n",
    "    \n",
    "\n",
    "def leave_one_subject_out(data, **kwargs):\n",
    "    \n",
    "    test_subject_id = kwargs[\"test_subject_id\"] if \"test_subject_id\" in kwargs else 1\n",
    "    \n",
    "    # get test data\n",
    "    # test_sub_idx = data.subject_ids.index(test_subject_id)\n",
    "    test_sub_idx = np.where(data.subject_ids == test_subject_id)[0][0]\n",
    "    selected_subject_data = data.data[test_sub_idx]\n",
    "    selected_subject_targets = data.targets[test_sub_idx]\n",
    "    test_dataset = PyTorchDataset(selected_subject_data, selected_subject_targets)\n",
    "    \n",
    "    # get train val data\n",
    "    indices = np.arange(data.data.shape[0])\n",
    "    train_val_data = data.data[indices!=test_sub_idx, :, :, :]\n",
    "    train_val_data = train_val_data.reshape((train_val_data.shape[0]*train_val_data.shape[1], train_val_data.shape[2], train_val_data.shape[3]))\n",
    "    train_val_targets = data.targets[indices!=test_sub_idx, :]\n",
    "    train_val_targets = train_val_targets.reshape((train_val_targets.shape[0]*train_val_targets.shape[1]))\n",
    "\n",
    "    train_dataset = PyTorchDataset(train_val_data, train_val_targets)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "data = MultipleSubjects(\n",
    "    dataset=HSSSVEP, \n",
    "    root=os.path.join(path, \"../data/hsssvep\"), \n",
    "    subject_ids=config.data.load_subject_ids, \n",
    "    func_preprocessing=func_preprocessing,\n",
    "    func_get_train_val_test_dataset=leave_one_subject_out,\n",
    "    verbose=True, \n",
    ")\n",
    "\n",
    "print(\"Final data shape:\", data.data.shape, data.targets.shape)\n",
    "\n",
    "num_channel = data.data.shape[2]\n",
    "num_classes = 40\n",
    "signal_length = data.data.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "running test_subject_id: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_subject_id': 30, 'mean_acc': 0.5, 'acc': []}\n",
      "\n",
      "running test_subject_id: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Global seed set to 1234\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_subject_id': 31, 'mean_acc': 0.8333333134651184, 'acc': []}\n",
      "\n",
      "running test_subject_id: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Global seed set to 1234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_subject_id': 32, 'mean_acc': 0.9833333492279053, 'acc': []}\n",
      "\n",
      "running test_subject_id: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Global seed set to 1234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_subject_id': 33, 'mean_acc': 0.28333333134651184, 'acc': []}\n",
      "\n",
      "running test_subject_id: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_subject_id': 34, 'mean_acc': 0.7875000238418579, 'acc': []}\n",
      "\n",
      "running test_subject_id: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_subject_id': 35, 'mean_acc': 0.7749999761581421, 'acc': []}\n",
      "\n",
      "mean all 0.6937499990065893\n"
     ]
    }
   ],
   "source": [
    "def train_test_subject(data, config, test_subject_id):\n",
    "    \n",
    "    ## init data\n",
    "    \n",
    "    train_dataset, test_dataset = data.get_train_val_test_dataset(test_subject_id=test_subject_id)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.training.batchsize, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.training.batchsize, shuffle=False)\n",
    "\n",
    "    ## init model\n",
    "    base_model = Deep4NetModel(\n",
    "        num_channel=num_channel,\n",
    "        num_classes=num_classes,\n",
    "        signal_length=signal_length,\n",
    "    )\n",
    "\n",
    "    model = LightningModelClassifier(\n",
    "        optimizer=config.model.optimizer,\n",
    "        scheduler=config.model.scheduler,\n",
    "        optimizer_learning_rate=config.training.learning_rate,\n",
    "        scheduler_warmup_epochs=config.training.num_warmup_epochs,\n",
    "    )\n",
    "    \n",
    "    model.build_model(model=base_model)\n",
    "\n",
    "    ## train\n",
    "\n",
    "    sub_dir = \"sub\"+ str(test_subject_id)\n",
    "    logger_tb = TensorBoardLogger(save_dir=\"tensorboard_logs\", name=config.run_name, sub_dir=sub_dir)\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "    trainer = Trainer(max_epochs=config.training.num_epochs, gpus=config.training.gpus, logger=logger_tb, progress_bar_refresh_rate=0, weights_summary=None, callbacks=[lr_monitor])\n",
    "    trainer.fit(model, train_loader)\n",
    "    \n",
    "    ## test\n",
    "    \n",
    "    result = trainer.test(dataloaders=test_loader, verbose=False)\n",
    "    test_acc = result[0]['test_acc_epoch']\n",
    "    \n",
    "    return test_acc\n",
    "\n",
    "####\n",
    "\n",
    "main_logger.write_to_log(\"Begin\", break_line=True)\n",
    "\n",
    "test_results_acc = {}\n",
    "means = []\n",
    "\n",
    "def k_fold_train_test_all_subjects():\n",
    "    \n",
    "    for test_subject_id in config.testing.test_subject_ids:\n",
    "        print()\n",
    "        print(\"running test_subject_id:\", test_subject_id)\n",
    "        \n",
    "        if test_subject_id not in test_results_acc:\n",
    "            test_results_acc[test_subject_id] = []\n",
    "            \n",
    "        mean_acc = train_test_subject(data, config, test_subject_id)\n",
    "\n",
    "        means.append(mean_acc)\n",
    "        \n",
    "        this_result = {\n",
    "            \"test_subject_id\": test_subject_id,\n",
    "            \"mean_acc\": mean_acc,\n",
    "            \"acc\": test_results_acc[test_subject_id],\n",
    "        }        \n",
    "        print(this_result)\n",
    "        main_logger.write_to_log(this_result)\n",
    "\n",
    "k_fold_train_test_all_subjects()\n",
    "\n",
    "mean_acc = np.mean(means)\n",
    "print()\n",
    "print(\"mean all\", mean_acc)\n",
    "main_logger.write_to_log(\"Mean acc: \"+str(mean_acc), break_line=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "running test_subject_id: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n"
     ]
    }
   ],
   "source": [
    "def train_test_subject(data, config, test_subject_id):\n",
    "    \n",
    "    ## init data\n",
    "    \n",
    "    train_dataset, test_dataset = data.get_train_val_test_dataset(test_subject_id=test_subject_id)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.training.batchsize, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.training.batchsize, shuffle=False)\n",
    "\n",
    "    ## init model\n",
    "    base_model = Deep4NetModel(\n",
    "        num_channel=num_channel,\n",
    "        num_classes=num_classes,\n",
    "        signal_length=signal_length,\n",
    "    )\n",
    "\n",
    "    model = LightningModelClassifier(\n",
    "        optimizer=config.model.optimizer,\n",
    "        scheduler=config.model.scheduler,\n",
    "        optimizer_learning_rate=config.training.learning_rate,\n",
    "        scheduler_warmup_epochs=config.training.num_warmup_epochs,\n",
    "    )\n",
    "    \n",
    "    model.build_model(model=base_model)\n",
    "\n",
    "    ## train\n",
    "\n",
    "    sub_dir = \"sub\"+ str(test_subject_id)\n",
    "    logger_tb = TensorBoardLogger(save_dir=\"tensorboard_logs\", name=config.run_name, sub_dir=sub_dir)\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "    trainer = Trainer(max_epochs=config.training.num_epochs, gpus=config.training.gpus, logger=logger_tb, progress_bar_refresh_rate=0, weights_summary=None, callbacks=[lr_monitor])\n",
    "    trainer.fit(model, train_loader)\n",
    "    \n",
    "    ## test\n",
    "    \n",
    "    result = trainer.test(dataloaders=test_loader, verbose=False)\n",
    "    test_acc = result[0]['test_acc_epoch']\n",
    "    \n",
    "    return test_acc\n",
    "\n",
    "####\n",
    "\n",
    "main_logger.write_to_log(\"Begin\", break_line=True)\n",
    "\n",
    "test_results_acc = {}\n",
    "means = []\n",
    "\n",
    "def k_fold_train_test_all_subjects():\n",
    "    \n",
    "    for test_subject_id in config.testing.test_subject_ids:\n",
    "        print()\n",
    "        print(\"running test_subject_id:\", test_subject_id)\n",
    "        \n",
    "        if test_subject_id not in test_results_acc:\n",
    "            test_results_acc[test_subject_id] = []\n",
    "            \n",
    "        mean_acc = train_test_subject(data, config, test_subject_id)\n",
    "\n",
    "        means.append(mean_acc)\n",
    "        \n",
    "        this_result = {\n",
    "            \"test_subject_id\": test_subject_id,\n",
    "            \"mean_acc\": mean_acc,\n",
    "            \"acc\": test_results_acc[test_subject_id],\n",
    "        }        \n",
    "        print(this_result)\n",
    "        main_logger.write_to_log(this_result)\n",
    "\n",
    "k_fold_train_test_all_subjects()\n",
    "\n",
    "mean_acc = np.mean(means)\n",
    "print()\n",
    "print(\"mean all\", mean_acc)\n",
    "main_logger.write_to_log(\"Mean acc: \"+str(mean_acc), break_line=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
