{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "cwd = os.getcwd()\n",
    "import sys\n",
    "path = os.path.join(cwd, \"..\\\\..\\\\\")\n",
    "sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchlibrosa.stft import ISTFT, STFT, magphase\n",
    "\n",
    "import pytorch_lightning\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "logging.getLogger('lightning').setLevel(0)\n",
    "warnings.filterwarnings('ignore')\n",
    "pytorch_lightning.utilities.distributed.log.setLevel(logging.ERROR)\n",
    "\n",
    "from splearn.data import MultipleSubjects, PyTorchDataset, PyTorchDataset2Views, HSSSVEP\n",
    "from splearn.filter.butterworth import butter_bandpass_filter\n",
    "from splearn.filter.notch import notch_filter\n",
    "from splearn.filter.channels import pick_channels\n",
    "from splearn.utils import Logger, Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1234"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    \"run_name\": \"ssl_hsssvep\",\n",
    "    \"data\": {\n",
    "        \"load_subject_ids\": np.arange(1,36),\n",
    "        \"selected_channels\": [\"PO8\", \"PZ\", \"PO7\", \"PO4\", \"POz\", \"PO3\", \"O2\", \"Oz\", \"O1\"],\n",
    "        \"input_channels\": 9,\n",
    "        \"target_sources_num\": 40,\n",
    "        \"sample_length\": 250,\n",
    "        \"num_classes\": 40\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"num_epochs\": 100,\n",
    "        \"num_warmup_epochs\": 10,\n",
    "        \"learning_rate\": 0.03,\n",
    "        # \"gpus\": torch.cuda.device_count(),\n",
    "        \"gpus\": [0],\n",
    "        \"batchsize\": 256\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"projection_size\": 1024,\n",
    "        \"optimizer\": \"adamw\",\n",
    "        \"scheduler\": \"cosine_with_warmup\",\n",
    "    },\n",
    "    \"testing\": {\n",
    "        \"test_subject_ids\": np.arange(33,34),\n",
    "        \"kfolds\": np.arange(0,3),\n",
    "    },\n",
    "    \"seed\": 1234\n",
    "}\n",
    "\n",
    "main_logger = Logger(filename_postfix=config[\"run_name\"])\n",
    "main_logger.write_to_log(\"Config\")\n",
    "main_logger.write_to_log(config)\n",
    "\n",
    "config = Config(config)\n",
    "\n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load subject: 1\n",
      "Load subject: 2\n",
      "Load subject: 3\n",
      "Load subject: 4\n",
      "Load subject: 5\n",
      "Load subject: 6\n",
      "Load subject: 7\n",
      "Load subject: 8\n",
      "Load subject: 9\n",
      "Load subject: 10\n",
      "Load subject: 11\n",
      "Load subject: 12\n",
      "Load subject: 13\n",
      "Load subject: 14\n",
      "Load subject: 15\n",
      "Load subject: 16\n",
      "Load subject: 17\n",
      "Load subject: 18\n",
      "Load subject: 19\n",
      "Load subject: 20\n",
      "Load subject: 21\n",
      "Load subject: 22\n",
      "Load subject: 23\n",
      "Load subject: 24\n",
      "Load subject: 25\n",
      "Load subject: 26\n",
      "Load subject: 27\n",
      "Load subject: 28\n",
      "Load subject: 29\n",
      "Load subject: 30\n",
      "Load subject: 31\n",
      "Load subject: 32\n",
      "Load subject: 33\n",
      "Load subject: 34\n",
      "Load subject: 35\n",
      "Final data shape: (35, 240, 9, 250)\n",
      "train_loader (5440, 9, 250) (5440,)\n",
      "val_loader (2720, 9, 250) (2720,)\n",
      "test_loader (240, 9, 250) (240,)\n"
     ]
    }
   ],
   "source": [
    "def onehot_targets(targets):\n",
    "    return (np.arange(targets.max()+1) == targets[...,None]).astype(int)\n",
    "\n",
    "\n",
    "def func_preprocessing(data):\n",
    "    data_x = data.data\n",
    "    # selected_channels = ['P7','P3','PZ','P4','P8','O1','Oz','O2','P1','P2','POz','PO3','PO4']\n",
    "    selected_channels = config.data.selected_channels\n",
    "    data_x = pick_channels(data_x, channel_names=data.channel_names, selected_channels=selected_channels)\n",
    "    # data_x = notch_filter(data_x, sampling_rate=data.sampling_rate, notch_freq=50.0)\n",
    "    data_x = butter_bandpass_filter(data_x, lowcut=4, highcut=75, sampling_rate=data.sampling_rate, order=6)\n",
    "    start_t = 125\n",
    "    end_t = 125 + 250\n",
    "    data_x = data_x[:,:,:,start_t:end_t]\n",
    "    data.set_data(data_x)\n",
    "\n",
    "\n",
    "def leave_one_subject_out(data, **kwargs):\n",
    "    \n",
    "    test_subject_id = kwargs[\"test_subject_id\"] if \"test_subject_id\" in kwargs else 1\n",
    "    kfold_k = kwargs[\"kfold_k\"] if \"kfold_k\" in kwargs else 0\n",
    "    kfold_split = kwargs[\"kfold_split\"] if \"kfold_split\" in kwargs else 3\n",
    "    \n",
    "    # get test data\n",
    "    # test_sub_idx = data.subject_ids.index(test_subject_id)\n",
    "    test_sub_idx = np.where(data.subject_ids == test_subject_id)[0][0]\n",
    "    selected_subject_data = data.data[test_sub_idx]\n",
    "    selected_subject_targets = data.targets[test_sub_idx]\n",
    "    # selected_subject_targets = onehot_targets(selected_subject_targets)\n",
    "    test_dataset = PyTorchDataset(selected_subject_data, selected_subject_targets)\n",
    "    # num_targets = selected_subject_targets.shape[1]\n",
    "\n",
    "    # get train val data\n",
    "    indices = np.arange(data.data.shape[0])\n",
    "    train_val_data = data.data[indices!=test_sub_idx, :, :, :]\n",
    "    \n",
    "    train_val_data = train_val_data.reshape((train_val_data.shape[0]*train_val_data.shape[1], train_val_data.shape[2], train_val_data.shape[3]))\n",
    "    train_val_targets = data.targets[indices!=test_sub_idx, :]\n",
    "    train_val_targets = train_val_targets.reshape((train_val_targets.shape[0]*train_val_targets.shape[1]))\n",
    "    \n",
    "    # train test split\n",
    "    (X_train, y_train), (X_val, y_val) = data.dataset_split_stratified(train_val_data, train_val_targets, k=kfold_k, n_splits=kfold_split)\n",
    "    # y_train = onehot_targets(y_train)\n",
    "    # y_val = onehot_targets(y_val)\n",
    "    # print(\"X_train.shape, X_val.shape\", X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "    \n",
    "    # create dataset\n",
    "    train_dataset = PyTorchDataset(X_train, y_train)\n",
    "    val_dataset = PyTorchDataset(X_val, y_val)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "data = MultipleSubjects(\n",
    "    dataset=HSSSVEP, \n",
    "    root=os.path.join(path, \"../data/hsssvep\"), \n",
    "    subject_ids=config.data.load_subject_ids, \n",
    "    func_preprocessing=func_preprocessing,\n",
    "    func_get_train_val_test_dataset=leave_one_subject_out,\n",
    "    verbose=True, \n",
    ")\n",
    "\n",
    "print(\"Final data shape:\", data.data.shape)\n",
    "\n",
    "test_subject_id = 33\n",
    "kfold_k = 0\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = data.get_train_val_test_dataset(test_subject_id=test_subject_id, kfold_k=kfold_k)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.training.batchsize, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.training.batchsize, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.training.batchsize, shuffle=False)\n",
    "\n",
    "print(\"train_loader\", train_loader.dataset.data.shape, train_loader.dataset.targets.shape)\n",
    "print(\"val_loader\", val_loader.dataset.data.shape, val_loader.dataset.targets.shape)\n",
    "print(\"test_loader\", test_loader.dataset.data.shape, test_loader.dataset.targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# class ResUNet143_Subbandtime(nn.Module, Base):\n",
    "#     def __init__(self, input_channels, target_sources_num):\n",
    "#         super(ResUNet143_Subbandtime, self).__init__()\n",
    "        \n",
    "#         self.input_channels = input_channels\n",
    "#         self.target_sources_num = target_sources_num\n",
    "\n",
    "#         window_size = 64\n",
    "#         hop_size = 25\n",
    "#         center = True\n",
    "#         pad_mode = \"reflect\"\n",
    "#         window = \"hann\"\n",
    "#         activation = \"leaky_relu\"\n",
    "#         momentum = 0.01\n",
    "\n",
    "#         self.subbands_num = 1 # 4\n",
    "#         self.K = 4  # outputs: |M|, cos∠M, sin∠M, Q\n",
    "\n",
    "#         self.downsample_ratio = 2 ** 3 # 5  # This number equals 2^{#encoder_blcoks}\n",
    "\n",
    "#         self.stft = STFT(\n",
    "#             n_fft=window_size,\n",
    "#             hop_length=hop_size,\n",
    "#             win_length=window_size,\n",
    "#             window=window,\n",
    "#             center=center,\n",
    "#             pad_mode=pad_mode,\n",
    "#             freeze_parameters=True,\n",
    "#         )\n",
    "\n",
    "#         self.istft = ISTFT(\n",
    "#             n_fft=window_size,\n",
    "#             hop_length=hop_size,\n",
    "#             win_length=window_size,\n",
    "#             window=window,\n",
    "#             center=center,\n",
    "#             pad_mode=pad_mode,\n",
    "#             freeze_parameters=True,\n",
    "#         )\n",
    "        \n",
    "#         self.bn0 = nn.BatchNorm2d(window_size // 2 + 1, momentum=momentum)\n",
    "        \n",
    "#         self.encoder_block1 = EncoderBlockRes4B(\n",
    "#             in_channels=input_channels,\n",
    "#             out_channels=32,\n",
    "#             kernel_size=(3, 3),\n",
    "#             downsample=(2, 2),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "        \n",
    "#         self.encoder_block2 = EncoderBlockRes4B(\n",
    "#             in_channels=32,\n",
    "#             out_channels=64,\n",
    "#             kernel_size=(3, 3),\n",
    "#             downsample=(2, 2),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "#         self.encoder_block3 = EncoderBlockRes4B(\n",
    "#             in_channels=64,\n",
    "#             out_channels=128,\n",
    "#             kernel_size=(3, 3),\n",
    "#             downsample=(2, 2),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "#         self.encoder_block4 = EncoderBlockRes4B(\n",
    "#             in_channels=128,\n",
    "#             out_channels=256,\n",
    "#             kernel_size=(3, 3),\n",
    "#             downsample=(2, 2),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "#         self.encoder_block5 = EncoderBlockRes4B(\n",
    "#             in_channels=256,\n",
    "#             out_channels=384,\n",
    "#             kernel_size=(3, 3),\n",
    "#             downsample=(2, 2),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "#         self.encoder_block6 = EncoderBlockRes4B(\n",
    "#             in_channels=384,\n",
    "#             out_channels=384,\n",
    "#             kernel_size=(3, 3),\n",
    "#             downsample=(1, 2),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "        \n",
    "#         conv_block_in_channels = 128 # 384\n",
    "        \n",
    "#         self.conv_block7a = EncoderBlockRes4B(\n",
    "#             in_channels=conv_block_in_channels,\n",
    "#             out_channels=conv_block_in_channels,\n",
    "#             kernel_size=(3, 3),\n",
    "#             downsample=(1, 1),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "#         self.conv_block7b = EncoderBlockRes4B(\n",
    "#             in_channels=conv_block_in_channels,\n",
    "#             out_channels=conv_block_in_channels,\n",
    "#             kernel_size=(3, 3),\n",
    "#             downsample=(1, 1),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "#         self.conv_block7c = EncoderBlockRes4B(\n",
    "#             in_channels=conv_block_in_channels,\n",
    "#             out_channels=conv_block_in_channels,\n",
    "#             kernel_size=(3, 3),\n",
    "#             downsample=(1, 1),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "#         self.conv_block7d = EncoderBlockRes4B(\n",
    "#             in_channels=conv_block_in_channels,\n",
    "#             out_channels=conv_block_in_channels,\n",
    "#             kernel_size=(3, 3),\n",
    "#             downsample=(1, 1),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "        \n",
    "#         self.decoder_block1 = DecoderBlockRes4B(\n",
    "#             in_channels=384,\n",
    "#             out_channels=384,\n",
    "#             kernel_size=(3, 3),\n",
    "#             upsample=(1, 2),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "#         self.decoder_block2 = DecoderBlockRes4B(\n",
    "#             in_channels=384,\n",
    "#             out_channels=384,\n",
    "#             kernel_size=(3, 3),\n",
    "#             upsample=(2, 2),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "#         self.decoder_block3 = DecoderBlockRes4B(\n",
    "#             in_channels=384,\n",
    "#             out_channels=256,\n",
    "#             kernel_size=(3, 3),\n",
    "#             upsample=(2, 2),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "#         self.decoder_block4 = DecoderBlockRes4B(\n",
    "#             in_channels=128,\n",
    "#             out_channels=128,\n",
    "#             kernel_size=(3, 3),\n",
    "#             upsample=(2, 2),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "#         self.decoder_block5 = DecoderBlockRes4B(\n",
    "#             in_channels=128,\n",
    "#             out_channels=64,\n",
    "#             kernel_size=(3, 3),\n",
    "#             upsample=(2, 2),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "#         self.decoder_block6 = DecoderBlockRes4B(\n",
    "#             in_channels=64,\n",
    "#             out_channels=32,\n",
    "#             kernel_size=(3, 3),\n",
    "#             upsample=(2, 2),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "\n",
    "#         self.after_conv_block1 = EncoderBlockRes4B(\n",
    "#             in_channels=32,\n",
    "#             out_channels=32,\n",
    "#             kernel_size=(3, 3),\n",
    "#             downsample=(1, 1),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "\n",
    "#         self.after_conv2 = nn.Conv2d(\n",
    "#             in_channels=32,\n",
    "#             out_channels=target_sources_num\n",
    "#             * input_channels\n",
    "#             * self.K\n",
    "#             * self.subbands_num,\n",
    "#             kernel_size=(1, 1),\n",
    "#             stride=(1, 1),\n",
    "#             padding=(0, 0),\n",
    "#             bias=True,\n",
    "#         )\n",
    "        \n",
    "#         self.out_conv_block = EncoderBlockRes4B(\n",
    "#             in_channels=target_sources_num\n",
    "#             * input_channels\n",
    "#             * self.subbands_num,\n",
    "#             out_channels=target_sources_num,\n",
    "#             kernel_size=(1, 1),\n",
    "#             downsample=(1, 1),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "\n",
    "#         self.init_weights()\n",
    "        \n",
    "#     def init_weights(self):\n",
    "#         init_bn(self.bn0)\n",
    "#         init_layer(self.after_conv2)\n",
    "        \n",
    "#     def feature_maps_to_wav(\n",
    "#         self,\n",
    "#         input_tensor: torch.Tensor,\n",
    "#         sp: torch.Tensor,\n",
    "#         sin_in: torch.Tensor,\n",
    "#         cos_in: torch.Tensor,\n",
    "#         audio_length: int,\n",
    "#     ) -> torch.Tensor:\n",
    "#         r\"\"\"Convert feature maps to waveform.\n",
    "#         Args:\n",
    "#             input_tensor: (batch_size, target_sources_num * input_channels * self.K, time_steps, freq_bins)\n",
    "#             sp: (batch_size, target_sources_num * input_channels, time_steps, freq_bins)\n",
    "#             sin_in: (batch_size, target_sources_num * input_channels, time_steps, freq_bins)\n",
    "#             cos_in: (batch_size, target_sources_num * input_channels, time_steps, freq_bins)\n",
    "#         Outputs:\n",
    "#             waveform: (batch_size, target_sources_num * input_channels, segment_samples)\n",
    "#         \"\"\"\n",
    "#         batch_size, _, time_steps, freq_bins = input_tensor.shape\n",
    "\n",
    "#         x = input_tensor.reshape(\n",
    "#             batch_size,\n",
    "#             self.target_sources_num,\n",
    "#             self.input_channels,\n",
    "#             self.K,\n",
    "#             time_steps,\n",
    "#             freq_bins,\n",
    "#         )\n",
    "#         # x: (batch_size, target_sources_num, input_channles, K, time_steps, freq_bins)\n",
    "\n",
    "#         mask_mag = torch.sigmoid(x[:, :, :, 0, :, :])\n",
    "#         _mask_real = torch.tanh(x[:, :, :, 1, :, :])\n",
    "#         _mask_imag = torch.tanh(x[:, :, :, 2, :, :])\n",
    "#         linear_mag = torch.tanh(x[:, :, :, 3, :, :])\n",
    "#         _, mask_cos, mask_sin = magphase(_mask_real, _mask_imag)\n",
    "#         # mask_cos, mask_sin: (batch_size, target_sources_num, input_channles, time_steps, freq_bins)\n",
    "\n",
    "#         # Y = |Y|cos∠Y + j|Y|sin∠Y\n",
    "#         #   = |Y|cos(∠X + ∠M) + j|Y|sin(∠X + ∠M)\n",
    "#         #   = |Y|(cos∠X cos∠M - sin∠X sin∠M) + j|Y|(sin∠X cos∠M + cos∠X sin∠M)\n",
    "#         out_cos = (\n",
    "#             cos_in[:, None, :, :, :] * mask_cos - sin_in[:, None, :, :, :] * mask_sin\n",
    "#         )\n",
    "#         out_sin = (\n",
    "#             sin_in[:, None, :, :, :] * mask_cos + cos_in[:, None, :, :, :] * mask_sin\n",
    "#         )\n",
    "#         # out_cos: (batch_size, target_sources_num, input_channles, time_steps, freq_bins)\n",
    "#         # out_sin: (batch_size, target_sources_num, input_channles, time_steps, freq_bins)\n",
    "\n",
    "#         # Calculate |Y|.\n",
    "#         out_mag = F.relu_(sp[:, None, :, :, :] * mask_mag + linear_mag)\n",
    "#         # out_mag: (batch_size, target_sources_num, input_channles, time_steps, freq_bins)\n",
    "\n",
    "#         # Calculate Y_{real} and Y_{imag} for ISTFT.\n",
    "#         out_real = out_mag * out_cos\n",
    "#         out_imag = out_mag * out_sin\n",
    "#         # out_real, out_imag: (batch_size, target_sources_num, input_channles, time_steps, freq_bins)\n",
    "\n",
    "#         # Reformat shape to (n, 1, time_steps, freq_bins) for ISTFT.\n",
    "#         shape = (\n",
    "#             batch_size * self.target_sources_num * self.input_channels,\n",
    "#             1,\n",
    "#             time_steps,\n",
    "#             freq_bins,\n",
    "#         )\n",
    "#         out_real = out_real.reshape(shape)\n",
    "#         out_imag = out_imag.reshape(shape)\n",
    "\n",
    "#         # ISTFT.\n",
    "#         x = self.istft(out_real, out_imag, audio_length)\n",
    "#         # (batch_size * target_sources_num * input_channels, segments_num)\n",
    "\n",
    "#         # Reshape.\n",
    "#         waveform = x.reshape(\n",
    "#             batch_size, self.target_sources_num * self.input_channels, audio_length\n",
    "#         )\n",
    "#         # (batch_size, target_sources_num * input_channels, segments_num)\n",
    "\n",
    "#         return waveform\n",
    "        \n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         subband_x = x\n",
    "\n",
    "#         mag, cos_in, sin_in = self.wav_to_spectrogram_phase(subband_x)\n",
    "#         # mag, cos_in, sin_in: (batch_size, input_channels * subbands_num, time_steps, freq_bins)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         # Batch normalize on individual frequency bins.\n",
    "#         x = mag.transpose(1, 3)\n",
    "#         x = self.bn0(x)\n",
    "#         x = x.transpose(1, 3)\n",
    "#         # (batch_size, input_channels * subbands_num, time_steps, freq_bins)\n",
    "        \n",
    "#         # Pad spectrogram to be evenly divided by downsample ratio.\n",
    "#         origin_len = x.shape[2]\n",
    "#         pad_len = (\n",
    "#             int(np.ceil(x.shape[2] / self.downsample_ratio)) * self.downsample_ratio\n",
    "#             - origin_len\n",
    "#         )\n",
    "#         x = F.pad(x, pad=(0, 0, 0, pad_len))\n",
    "#         # x: (batch_size, input_channels * subbands_num, padded_time_steps, freq_bins)\n",
    "        \n",
    "#         # Let frequency bins be evenly divided by 2, e.g., 257 -> 256\n",
    "#         x = x[..., 0 : x.shape[-1] - 1]  # (bs, input_channels, T, F)\n",
    "#         # x: (batch_size, input_channels * subbands_num, padded_time_steps, freq_bins)\n",
    "        \n",
    "#         # UNet\n",
    "#         print(\"x\", x.shape)\n",
    "#         (x1_pool, x1) = self.encoder_block1(x)  # x1_pool: (bs, 32, T / 2, F / 2)\n",
    "#         # print(x1_pool.shape, x1.shape)\n",
    "#         (x2_pool, x2) = self.encoder_block2(x1_pool)  # x2_pool: (bs, 64, T / 4, F / 4)\n",
    "#         # print(x2_pool.shape, x2.shape)\n",
    "#         (x3_pool, x3) = self.encoder_block3(x2_pool)  # x3_pool: (bs, 128, T / 8, F / 8)\n",
    "#         # print(x3_pool.shape, x3.shape)\n",
    "#         # (x4_pool, x4) = self.encoder_block4(x3_pool)  # x4_pool: (bs, 256, T / 16, F / 16)\n",
    "#         # (x5_pool, x5) = self.encoder_block5(x4_pool)  # x5_pool: (bs, 384, T / 32, F / 32)\n",
    "#         # (x6_pool, x6) = self.encoder_block6(x5_pool)  # x6_pool: (bs, 384, T / 32, F / 64)\n",
    "#         (x_center, _) = self.conv_block7a(x3_pool)  # (bs, 384, T / 32, F / 64)\n",
    "#         (x_center, _) = self.conv_block7b(x_center)  # (bs, 384, T / 32, F / 64)\n",
    "#         # (x_center, _) = self.conv_block7c(x_center)  # (bs, 384, T / 32, F / 64)\n",
    "#         # (x_center, _) = self.conv_block7d(x_center)  # (bs, 384, T / 32, F / 64)\n",
    "#         # x7 = self.decoder_block1(x_center, x6)  # (bs, 384, T / 32, F / 32)\n",
    "#         # x8 = self.decoder_block2(x7, x5)  # (bs, 384, T / 16, F / 16)\n",
    "#         # x9 = self.decoder_block3(x8, x4)  # (bs, 256, T / 8, F / 8)\n",
    "#         # print(\"x_center.shape, x3.shape\", x_center.shape, x3.shape)\n",
    "#         x10 = self.decoder_block4(x_center, x3)  # (bs, 128, T / 4, F / 4)\n",
    "#         x11 = self.decoder_block5(x10, x2)  # (bs, 64, T / 2, F / 2)\n",
    "#         x12 = self.decoder_block6(x11, x1)  # (bs, 32, T, F)\n",
    "#         print(\"x12\", x12.shape)\n",
    "        \n",
    "#         (x, _) = self.after_conv_block1(x12)  # (bs, 32, T, F)\n",
    "        \n",
    "#         x = self.after_conv2(x)\n",
    "#         # (batch_size, subbands_num * target_sources_num * input_channles * self.K, T, F')\n",
    "#         # print(33, \"x.shape\", x.shape)\n",
    "\n",
    "#         # Recover shape\n",
    "#         x = F.pad(x, pad=(0, 1))  # Pad frequency, e.g., 256 -> 257.\n",
    "\n",
    "#         x = x[:, :, 0:origin_len, :]\n",
    "#         # (batch_size, subbands_num * target_sources_num * input_channles * self.K, T, F')\n",
    "#         print(99, x.shape)\n",
    "#         audio_length = subband_x.shape[2]\n",
    "        \n",
    "#         # Recover each subband spectrograms to subband waveforms. Then synthesis\n",
    "#         # the subband waveforms to a waveform.\n",
    "#         C1 = x.shape[1] // self.subbands_num\n",
    "#         C2 = mag.shape[1] // self.subbands_num\n",
    "\n",
    "#         separated_subband_audio = torch.cat(\n",
    "#             [\n",
    "#                 self.feature_maps_to_wav(\n",
    "#                     input_tensor=x[:, j * C1 : (j + 1) * C1, :, :],\n",
    "#                     sp=mag[:, j * C2 : (j + 1) * C2, :, :],\n",
    "#                     sin_in=sin_in[:, j * C2 : (j + 1) * C2, :, :],\n",
    "#                     cos_in=cos_in[:, j * C2 : (j + 1) * C2, :, :],\n",
    "#                     audio_length=audio_length,\n",
    "#                 )\n",
    "#                 for j in range(self.subbands_num)\n",
    "#             ],\n",
    "#             dim=1,\n",
    "#         )\n",
    "#         # （batch_size, subbands_num * target_sources_num * input_channles, segment_samples)\n",
    "        \n",
    "#         separated_subband_audio = torch.unsqueeze(separated_subband_audio, 2)\n",
    "#         (y, _) = self.out_conv_block(separated_subband_audio)\n",
    "        \n",
    "#         y = torch.squeeze(y, 2)\n",
    "        \n",
    "#         return y\n",
    "        \n",
    "        \n",
    "        \n",
    "# tmp_x = torch.rand(3, 9, 1000)\n",
    "# # \n",
    "# # input_dict = {\n",
    "# #     \"waveform\": tmp_x\n",
    "# # }\n",
    "\n",
    "# tmp_layer = ResUNet143_Subbandtime(input_channels=9, target_sources_num=10)\n",
    "# tmp_y = tmp_layer(tmp_x)\n",
    "# tmp_y.shape\n",
    "\n",
    "# # torch.Size([3, 9, 10, 33]) torch.Size([3, 9, 10, 33]) torch.Size([3, 9, 10, 33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(layer: nn.Module):\n",
    "    r\"\"\"Initialize a Linear or Convolutional layer.\"\"\"\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.0)\n",
    "            \n",
    "def init_bn(bn: nn.Module):\n",
    "    r\"\"\"Initialize a Batchnorm layer.\"\"\"\n",
    "    bn.bias.data.fill_(0.0)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "    bn.running_mean.data.fill_(0.0)\n",
    "    bn.running_var.data.fill_(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Common 2D convolutions\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn.utils import weight_norm\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple, List\n",
    "\n",
    "from splearn.nn.modules.functional import Swish\n",
    "from splearn.nn.utils import get_class_name\n",
    "\n",
    "\n",
    "class Conv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: 4-dim tensor\n",
    "        Shape [batch, in_channels, H, W]\n",
    "    Return: 4-dim tensor\n",
    "        Shape [batch, out_channels, H, W]\n",
    "        \n",
    "    Args:\n",
    "        in_channels : int\n",
    "            Should match input `channel`\n",
    "        out_channels : int\n",
    "            Return tensor with `out_channels`\n",
    "        kernel_size : int or 2-dim tuple\n",
    "        stride : int or 2-dim tuple, default: 1\n",
    "        padding : int or 2-dim tuple or True\n",
    "            Apply `padding` if given int or 2-dim tuple. Perform TensorFlow-like 'SAME' padding if True\n",
    "        dilation : int or 2-dim tuple, default: 1\n",
    "        groups : int or 2-dim tuple, default: 1\n",
    "        w_in: int, optional\n",
    "            The size of `W` axis. If given, `w_out` is available.\n",
    "    \n",
    "    Usage:\n",
    "        x = torch.randn(1, 22, 1, 256)\n",
    "        conv1 = Conv2dSamePadding(22, 64, kernel_size=17, padding=True, w_in=256)\n",
    "        y = conv1(x)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=\"SAME\", dilation=1, groups=1, w_in=None, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        padding = padding\n",
    "        self.kernel_size = kernel_size = kernel_size\n",
    "        self.stride = stride = stride\n",
    "        self.dilation = dilation = dilation\n",
    "        \n",
    "        self.padding_same = False\n",
    "        if padding == \"SAME\":\n",
    "            self.padding_same = True\n",
    "            padding = (0,0)\n",
    "        \n",
    "        if isinstance(padding, int):\n",
    "            padding = (padding, padding)\n",
    "            \n",
    "        if isinstance(kernel_size, int):\n",
    "            self.kernel_size = kernel_size = (kernel_size, kernel_size)\n",
    "            \n",
    "        if isinstance(stride, int):\n",
    "            self.stride = stride = (stride, stride)\n",
    "        \n",
    "        if isinstance(dilation, int):\n",
    "            self.dilation = dilation = (dilation, dilation)\n",
    "            \n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size=kernel_size, \n",
    "            stride=stride, \n",
    "            padding=0 if padding==True else padding, \n",
    "            dilation=dilation, \n",
    "            groups=groups,\n",
    "            bias=bias\n",
    "        )\n",
    "        \n",
    "        self.weight = self.conv.weight\n",
    "        \n",
    "        if w_in is not None:\n",
    "            self.w_out = int( ((w_in + 2 * padding[1] - dilation[1] * (kernel_size[1]-1)-1) / 1) + 1 )\n",
    "        if self.padding_same == \"SAME\": # if SAME, then replace, w_out = w_in, obviously\n",
    "            self.w_out = w_in\n",
    "            \n",
    "    def forward(self, x):\n",
    "        if self.padding_same == True:\n",
    "            x = self.pad_same(x, self.kernel_size, self.stride, self.dilation)\n",
    "        return self.conv(x)\n",
    "    \n",
    "    # Calculate asymmetric TensorFlow-like 'SAME' padding for a convolution\n",
    "    def get_same_padding(self, x: int, k: int, s: int, d: int):\n",
    "        return max((math.ceil(x / s) - 1) * s + (k - 1) * d + 1 - x, 0)\n",
    "\n",
    "    # Dynamically pad input x with 'SAME' padding for conv with specified args\n",
    "    def pad_same(self, x, k: List[int], s: List[int], d: List[int] = (1, 1), value: float = 0):\n",
    "        ih, iw = x.size()[-2:]\n",
    "        pad_h, pad_w = self.get_same_padding(ih, k[0], s[0], d[0]), self.get_same_padding(iw, k[1], s[1], d[1])\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2], value=value)\n",
    "        return x\n",
    "    \n",
    "######\n",
    "\n",
    "class ConvBlockRes(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, activation, momentum):\n",
    "        r\"\"\"Residual block.\"\"\"\n",
    "        super(ConvBlockRes, self).__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        \n",
    "        padding = [kernel_size[0] // 2, kernel_size[1] // 2]\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels, momentum=momentum)\n",
    "        # self.bn2 = nn.BatchNorm2d(out_channels, momentum=momentum)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=(1, 1),\n",
    "            dilation=(1, 1),\n",
    "            padding=padding,\n",
    "            bias=False,\n",
    "        )\n",
    "        \n",
    "        # self.conv2 = nn.Conv2d(\n",
    "        #     in_channels=out_channels,\n",
    "        #     out_channels=out_channels,\n",
    "        #     kernel_size=kernel_size,\n",
    "        #     stride=(1, 1),\n",
    "        #     dilation=(1, 1),\n",
    "        #     padding=padding,\n",
    "        #     bias=False,\n",
    "        # )\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=(1, 1),\n",
    "                stride=(1, 1),\n",
    "                padding=(0, 0),\n",
    "            )\n",
    "            self.is_shortcut = True\n",
    "        else:\n",
    "            self.is_shortcut = False\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_bn(self.bn1)\n",
    "        # init_bn(self.bn2)\n",
    "        init_layer(self.conv1)\n",
    "        # init_layer(self.conv2)\n",
    "\n",
    "        if self.is_shortcut:\n",
    "            init_layer(self.shortcut)\n",
    "\n",
    "    def forward(self, x):\n",
    "        origin = x\n",
    "        x = self.conv1(F.leaky_relu_(self.bn1(x), negative_slope=0.01))\n",
    "        # x = self.conv2(F.leaky_relu_(self.bn2(x), negative_slope=0.01))\n",
    "\n",
    "        if self.is_shortcut:\n",
    "            x1 = self.shortcut(origin)            \n",
    "            return x1 + x\n",
    "        else:\n",
    "            return origin + x\n",
    "        \n",
    "        \n",
    "# in_channels=384,\n",
    "# out_channels=384,\n",
    "\n",
    "\n",
    "# activation = \"leaky_relu\"\n",
    "# momentum = 0.01\n",
    "# tmp_layer = ConvBlockRes(in_channels=9, out_channels=9, kernel_size=(3,3), activation=activation, momentum=momentum)\n",
    "# tmp_x = torch.rand(3, 9, 240, 240)\n",
    "# tmp_y = tmp_layer(tmp_x)\n",
    "# tmp_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List#, NoReturn\n",
    "\n",
    "\n",
    "class Base:\n",
    "    def __init__(self):\n",
    "        r\"\"\"Base function for extracting spectrogram, cos, and sin, etc.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def spectrogram(self, input: torch.Tensor, eps: float = 0.0) -> torch.Tensor:\n",
    "        r\"\"\"Calculate spectrogram.\n",
    "        Args:\n",
    "            input: (batch_size, segments_num)\n",
    "            eps: float\n",
    "        Returns:\n",
    "            spectrogram: (batch_size, time_steps, freq_bins)\n",
    "        \"\"\"\n",
    "        (real, imag) = self.stft(input)\n",
    "        return torch.clamp(real ** 2 + imag ** 2, eps, np.inf) ** 0.5\n",
    "\n",
    "    def spectrogram_phase(\n",
    "        self, input: torch.Tensor, eps: float = 0.0\n",
    "    ) -> List[torch.Tensor]:\n",
    "        r\"\"\"Calculate the magnitude, cos, and sin of the STFT of input.\n",
    "        Args:\n",
    "            input: (batch_size, segments_num)\n",
    "            eps: float\n",
    "        Returns:\n",
    "            mag: (batch_size, time_steps, freq_bins)\n",
    "            cos: (batch_size, time_steps, freq_bins)\n",
    "            sin: (batch_size, time_steps, freq_bins)\n",
    "        \"\"\"\n",
    "        (real, imag) = self.stft(input)\n",
    "        mag = torch.clamp(real ** 2 + imag ** 2, eps, np.inf) ** 0.5\n",
    "        cos = real / mag\n",
    "        sin = imag / mag\n",
    "        return mag, cos, sin\n",
    "\n",
    "    def wav_to_spectrogram_phase(\n",
    "        self, input: torch.Tensor, eps: float = 1e-10\n",
    "    ) -> List[torch.Tensor]:\n",
    "        r\"\"\"Convert waveforms to magnitude, cos, and sin of STFT.\n",
    "        Args:\n",
    "            input: (batch_size, channels_num, segment_samples)\n",
    "            eps: float\n",
    "        Outputs:\n",
    "            mag: (batch_size, channels_num, time_steps, freq_bins)\n",
    "            cos: (batch_size, channels_num, time_steps, freq_bins)\n",
    "            sin: (batch_size, channels_num, time_steps, freq_bins)\n",
    "        \"\"\"\n",
    "        batch_size, channels_num, segment_samples = input.shape\n",
    "\n",
    "        # Reshape input with shapes of (n, segments_num) to meet the\n",
    "        # requirements of the stft function.\n",
    "        x = input.reshape(batch_size * channels_num, segment_samples)\n",
    "\n",
    "        mag, cos, sin = self.spectrogram_phase(x, eps=eps)\n",
    "        # mag, cos, sin: (batch_size * channels_num, 1, time_steps, freq_bins)\n",
    "\n",
    "        _, _, time_steps, freq_bins = mag.shape\n",
    "        mag = mag.reshape(batch_size, channels_num, time_steps, freq_bins)\n",
    "        cos = cos.reshape(batch_size, channels_num, time_steps, freq_bins)\n",
    "        sin = sin.reshape(batch_size, channels_num, time_steps, freq_bins)\n",
    "\n",
    "        return mag, cos, sin\n",
    "\n",
    "    def wav_to_spectrogram(\n",
    "        self, input: torch.Tensor, eps: float = 1e-10\n",
    "    ) -> List[torch.Tensor]:\n",
    "\n",
    "        mag, cos, sin = self.wav_to_spectrogram_phase(input, eps)\n",
    "        return mag\n",
    "    \n",
    "    \n",
    "class EncoderBlockRes4B(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size, downsample, activation, momentum, \n",
    "    ):\n",
    "        r\"\"\"Encoder block, contains 8 convolutional layers.\"\"\"\n",
    "        super(EncoderBlockRes4B, self).__init__()\n",
    "\n",
    "        self.conv_block1 = ConvBlockRes(\n",
    "            in_channels, out_channels, kernel_size, activation, momentum,\n",
    "        )\n",
    "        self.conv_block2 = ConvBlockRes(\n",
    "            out_channels, out_channels, kernel_size, activation, momentum,\n",
    "        )\n",
    "        # self.conv_block3 = ConvBlockRes(\n",
    "        #     out_channels, out_channels, kernel_size, activation, momentum\n",
    "        # )\n",
    "        # self.conv_block4 = ConvBlockRes(\n",
    "        #     out_channels, out_channels, kernel_size, activation, momentum\n",
    "        # )\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoder = self.conv_block1(x)\n",
    "        encoder = self.conv_block2(encoder)\n",
    "        # encoder = self.conv_block3(encoder)\n",
    "        # encoder = self.conv_block4(encoder)\n",
    "        encoder_pool = F.avg_pool2d(encoder, kernel_size=self.downsample)\n",
    "        return encoder_pool, encoder\n",
    "    \n",
    "class DecoderBlockRes4B(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size, upsample, activation, momentum\n",
    "    ):\n",
    "        r\"\"\"Decoder block, contains 1 transpose convolutional and 8 convolutional layers.\"\"\"\n",
    "        super(DecoderBlockRes4B, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = upsample\n",
    "        self.activation = activation\n",
    "\n",
    "        self.conv1 = torch.nn.ConvTranspose2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=self.stride,\n",
    "            stride=self.stride,\n",
    "            padding=(0, 0),\n",
    "            bias=False,\n",
    "            dilation=(1, 1),\n",
    "        )\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels, momentum=momentum)\n",
    "        self.conv_block2 = ConvBlockRes(\n",
    "            out_channels * 2, out_channels, kernel_size, activation, momentum\n",
    "        )\n",
    "        # self.conv_block3 = ConvBlockRes(\n",
    "        #     out_channels, out_channels, kernel_size, activation, momentum\n",
    "        # )\n",
    "        # self.conv_block4 = ConvBlockRes(\n",
    "        #     out_channels, out_channels, kernel_size, activation, momentum\n",
    "        # )\n",
    "        # self.conv_block5 = ConvBlockRes(\n",
    "        #     out_channels, out_channels, kernel_size, activation, momentum\n",
    "        # )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_bn(self.bn1)\n",
    "        init_layer(self.conv1)\n",
    "\n",
    "    def forward(self, input_tensor, concat_tensor):\n",
    "        x = self.conv1(F.relu_(self.bn1(input_tensor)))\n",
    "        x = torch.cat((x, concat_tensor), dim=1)\n",
    "        x = self.conv_block2(x)\n",
    "        # x = self.conv_block3(x)\n",
    "        # x = self.conv_block4(x)\n",
    "        # x = self.conv_block5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "class MyModel(nn.Module, Base):\n",
    "    def __init__(self, input_channels, target_sources_num):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.target_sources_num = target_sources_num\n",
    "        \n",
    "        signal_length = 250\n",
    "\n",
    "        window_size = 64\n",
    "        hop_size = 25\n",
    "        center = True\n",
    "        pad_mode = \"reflect\"\n",
    "        window = \"hann\"\n",
    "        activation = \"leaky_relu\"\n",
    "        momentum = 0.01\n",
    "\n",
    "        self.subbands_num = 1 # 4\n",
    "        self.K = 4  # outputs: |M|, cos∠M, sin∠M, Q\n",
    "\n",
    "        self.downsample_ratio = 2 ** 3 # 5  # This number equals 2^{#encoder_blcoks}\n",
    "\n",
    "        self.stft = STFT(\n",
    "            n_fft=window_size,\n",
    "            hop_length=hop_size,\n",
    "            win_length=window_size,\n",
    "            window=window,\n",
    "            center=center,\n",
    "            pad_mode=pad_mode,\n",
    "            freeze_parameters=True,\n",
    "        )\n",
    "\n",
    "        self.istft = ISTFT(\n",
    "            n_fft=window_size,\n",
    "            hop_length=hop_size,\n",
    "            win_length=window_size,\n",
    "            window=window,\n",
    "            center=center,\n",
    "            pad_mode=pad_mode,\n",
    "            freeze_parameters=True,\n",
    "        )\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm2d(window_size // 2 + 1, momentum=momentum)\n",
    "        \n",
    "        self.encoder_block1 = EncoderBlockRes4B(\n",
    "            in_channels=input_channels,\n",
    "            out_channels=16,\n",
    "            kernel_size=(3, 3),\n",
    "            downsample=(2, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        \n",
    "        self.encoder_block2 = EncoderBlockRes4B(\n",
    "            in_channels=16,\n",
    "            out_channels=32,\n",
    "            kernel_size=(3, 3),\n",
    "            downsample=(2, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        \n",
    "        self.encoder_block3 = EncoderBlockRes4B(\n",
    "            in_channels=32,\n",
    "            out_channels=64,\n",
    "            kernel_size=(3, 3),\n",
    "            downsample=(2, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.encoder_block4 = EncoderBlockRes4B(\n",
    "            in_channels=64,\n",
    "            out_channels=128,\n",
    "            kernel_size=(3, 3),\n",
    "            downsample=(2, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "#         self.encoder_block4 = EncoderBlockRes4B(\n",
    "#             in_channels=128,\n",
    "#             out_channels=256,\n",
    "#             kernel_size=(3, 3),\n",
    "#             downsample=(2, 2),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "#         self.encoder_block5 = EncoderBlockRes4B(\n",
    "#             in_channels=256,\n",
    "#             out_channels=384,\n",
    "#             kernel_size=(3, 3),\n",
    "#             downsample=(2, 2),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "#         self.encoder_block6 = EncoderBlockRes4B(\n",
    "#             in_channels=384,\n",
    "#             out_channels=384,\n",
    "#             kernel_size=(3, 3),\n",
    "#             downsample=(1, 2),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "        \n",
    "#         conv_block_in_channels = 128 # 384\n",
    "        \n",
    "#         self.conv_block7a = EncoderBlockRes4B(\n",
    "#             in_channels=conv_block_in_channels,\n",
    "#             out_channels=conv_block_in_channels,\n",
    "#             kernel_size=(3, 3),\n",
    "#             downsample=(1, 1),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "#         self.conv_block7b = EncoderBlockRes4B(\n",
    "#             in_channels=conv_block_in_channels,\n",
    "#             out_channels=conv_block_in_channels,\n",
    "#             kernel_size=(3, 3),\n",
    "#             downsample=(1, 1),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "#         self.conv_block7c = EncoderBlockRes4B(\n",
    "#             in_channels=conv_block_in_channels,\n",
    "#             out_channels=conv_block_in_channels,\n",
    "#             kernel_size=(3, 3),\n",
    "#             downsample=(1, 1),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "#         self.conv_block7d = EncoderBlockRes4B(\n",
    "#             in_channels=conv_block_in_channels,\n",
    "#             out_channels=conv_block_in_channels,\n",
    "#             kernel_size=(3, 3),\n",
    "#             downsample=(1, 1),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "        \n",
    "#         self.decoder_block1 = DecoderBlockRes4B(\n",
    "#             in_channels=384,\n",
    "#             out_channels=384,\n",
    "#             kernel_size=(3, 3),\n",
    "#             upsample=(1, 2),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "#         self.decoder_block2 = DecoderBlockRes4B(\n",
    "#             in_channels=384,\n",
    "#             out_channels=384,\n",
    "#             kernel_size=(3, 3),\n",
    "#             upsample=(2, 2),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "#         self.decoder_block3 = DecoderBlockRes4B(\n",
    "#             in_channels=384,\n",
    "#             out_channels=256,\n",
    "#             kernel_size=(3, 3),\n",
    "#             upsample=(2, 2),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "#         self.decoder_block4 = DecoderBlockRes4B(\n",
    "#             in_channels=128,\n",
    "#             out_channels=128,\n",
    "#             kernel_size=(3, 3),\n",
    "#             upsample=(2, 2),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "#         self.decoder_block5 = DecoderBlockRes4B(\n",
    "#             in_channels=128,\n",
    "#             out_channels=64,\n",
    "#             kernel_size=(3, 3),\n",
    "#             upsample=(2, 2),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "#         self.decoder_block6 = DecoderBlockRes4B(\n",
    "#             in_channels=64,\n",
    "#             out_channels=32,\n",
    "#             kernel_size=(3, 3),\n",
    "#             upsample=(2, 2),\n",
    "#             activation=activation,\n",
    "#             momentum=momentum,\n",
    "#         )\n",
    "\n",
    "        self.after_conv_block1 = EncoderBlockRes4B(\n",
    "            in_channels=64,\n",
    "            out_channels=32,\n",
    "            kernel_size=(3, 3),\n",
    "            downsample=(1, 1),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "\n",
    "        self.after_conv2 = nn.Conv2d(\n",
    "            in_channels=32,\n",
    "            out_channels=target_sources_num\n",
    "            * input_channels\n",
    "            * self.K\n",
    "            * self.subbands_num,\n",
    "            kernel_size=(1, 1),\n",
    "            stride=(1, 1),\n",
    "            padding=(0, 0),\n",
    "            bias=True,\n",
    "        )\n",
    "        \n",
    "        # self.out_conv_block = EncoderBlockRes4B(\n",
    "        #     in_channels=target_sources_num\n",
    "        #     * input_channels\n",
    "        #     * self.subbands_num,\n",
    "        #     out_channels=target_sources_num,\n",
    "        #     kernel_size=(1, signal_length),\n",
    "        #     downsample=(1, 1),\n",
    "        #     activation=activation,\n",
    "        #     momentum=momentum,\n",
    "        #     padding=(0,0),\n",
    "        #     shortcut=False\n",
    "        # )\n",
    "        \n",
    "        self.out_conv_block = Conv2d(\n",
    "            in_channels=target_sources_num\n",
    "            * input_channels\n",
    "            * self.subbands_num,\n",
    "            out_channels=target_sources_num,\n",
    "            kernel_size=(1, signal_length),\n",
    "            stride=(1, 1),\n",
    "            dilation=(1, 1),\n",
    "            padding=(0,0),\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.after_conv2)\n",
    "        \n",
    "    def feature_maps_to_wav(\n",
    "        self,\n",
    "        input_tensor: torch.Tensor,\n",
    "        sp: torch.Tensor,\n",
    "        sin_in: torch.Tensor,\n",
    "        cos_in: torch.Tensor,\n",
    "        audio_length: int,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"Convert feature maps to waveform.\n",
    "        Args:\n",
    "            input_tensor: (batch_size, target_sources_num * input_channels * self.K, time_steps, freq_bins)\n",
    "            sp: (batch_size, target_sources_num * input_channels, time_steps, freq_bins)\n",
    "            sin_in: (batch_size, target_sources_num * input_channels, time_steps, freq_bins)\n",
    "            cos_in: (batch_size, target_sources_num * input_channels, time_steps, freq_bins)\n",
    "        Outputs:\n",
    "            waveform: (batch_size, target_sources_num * input_channels, segment_samples)\n",
    "        \"\"\"\n",
    "        batch_size, _, time_steps, freq_bins = input_tensor.shape\n",
    "\n",
    "        x = input_tensor.reshape(\n",
    "            batch_size,\n",
    "            self.target_sources_num,\n",
    "            self.input_channels,\n",
    "            self.K,\n",
    "            time_steps,\n",
    "            freq_bins,\n",
    "        )\n",
    "        # x: (batch_size, target_sources_num, input_channles, K, time_steps, freq_bins)\n",
    "\n",
    "        mask_mag = torch.sigmoid(x[:, :, :, 0, :, :])\n",
    "        _mask_real = torch.tanh(x[:, :, :, 1, :, :])\n",
    "        _mask_imag = torch.tanh(x[:, :, :, 2, :, :])\n",
    "        linear_mag = torch.tanh(x[:, :, :, 3, :, :])\n",
    "        _, mask_cos, mask_sin = magphase(_mask_real, _mask_imag)\n",
    "        # mask_cos, mask_sin: (batch_size, target_sources_num, input_channles, time_steps, freq_bins)\n",
    "\n",
    "        # Y = |Y|cos∠Y + j|Y|sin∠Y\n",
    "        #   = |Y|cos(∠X + ∠M) + j|Y|sin(∠X + ∠M)\n",
    "        #   = |Y|(cos∠X cos∠M - sin∠X sin∠M) + j|Y|(sin∠X cos∠M + cos∠X sin∠M)\n",
    "        out_cos = (\n",
    "            cos_in[:, None, :, :, :] * mask_cos - sin_in[:, None, :, :, :] * mask_sin\n",
    "        )\n",
    "        out_sin = (\n",
    "            sin_in[:, None, :, :, :] * mask_cos + cos_in[:, None, :, :, :] * mask_sin\n",
    "        )\n",
    "        # out_cos: (batch_size, target_sources_num, input_channles, time_steps, freq_bins)\n",
    "        # out_sin: (batch_size, target_sources_num, input_channles, time_steps, freq_bins)\n",
    "\n",
    "        # Calculate |Y|.\n",
    "        out_mag = F.relu_(sp[:, None, :, :, :] * mask_mag + linear_mag)\n",
    "        # out_mag: (batch_size, target_sources_num, input_channles, time_steps, freq_bins)\n",
    "\n",
    "        # Calculate Y_{real} and Y_{imag} for ISTFT.\n",
    "        out_real = out_mag * out_cos\n",
    "        out_imag = out_mag * out_sin\n",
    "        # out_real, out_imag: (batch_size, target_sources_num, input_channles, time_steps, freq_bins)\n",
    "\n",
    "        # Reformat shape to (n, 1, time_steps, freq_bins) for ISTFT.\n",
    "        shape = (\n",
    "            batch_size * self.target_sources_num * self.input_channels,\n",
    "            1,\n",
    "            time_steps,\n",
    "            freq_bins,\n",
    "        )\n",
    "        out_real = out_real.reshape(shape)\n",
    "        out_imag = out_imag.reshape(shape)\n",
    "\n",
    "        # ISTFT.\n",
    "        x = self.istft(out_real, out_imag, audio_length)\n",
    "        # (batch_size * target_sources_num * input_channels, segments_num)\n",
    "\n",
    "        # Reshape.\n",
    "        waveform = x.reshape(\n",
    "            batch_size, self.target_sources_num * self.input_channels, audio_length\n",
    "        )\n",
    "        # (batch_size, target_sources_num * input_channels, segments_num)\n",
    "\n",
    "        return waveform\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        subband_x = x\n",
    "\n",
    "        mag, cos_in, sin_in = self.wav_to_spectrogram_phase(subband_x)\n",
    "        # mag, cos_in, sin_in: (batch_size, input_channels * subbands_num, time_steps, freq_bins)\n",
    "        \n",
    "        # Batch normalize on individual frequency bins.\n",
    "        x = mag.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        # (batch_size, input_channels * subbands_num, time_steps, freq_bins)\n",
    "        # print(11, x.shape)\n",
    "        \n",
    "        # Pad spectrogram to be evenly divided by downsample ratio.\n",
    "        origin_len = x.shape[2]\n",
    "        # print(22, origin_len)\n",
    "        pad_len = (\n",
    "            int(np.ceil(x.shape[2] / self.downsample_ratio)) * self.downsample_ratio\n",
    "            - origin_len\n",
    "        )\n",
    "        x = F.pad(x, pad=(0, 0, 0, pad_len))\n",
    "        # print(33, x.shape)\n",
    "        # x: (batch_size, input_channels * subbands_num, padded_time_steps, freq_bins)\n",
    "        \n",
    "        # Let frequency bins be evenly divided by 2, e.g., 257 -> 256\n",
    "        x = x[..., 0 : x.shape[-1] - 1]  # (bs, input_channels, T, F)\n",
    "        # x: (batch_size, input_channels * subbands_num, padded_time_steps, freq_bins)\n",
    "        # print(44, x.shape)\n",
    "        \n",
    "        (x1_pool, x1) = self.encoder_block1(x)\n",
    "        (x2_pool, x2) = self.encoder_block2(x1)\n",
    "        (x3_pool, x3) = self.encoder_block3(x2)\n",
    "        \n",
    "        \n",
    "        (x, _) = self.after_conv_block1(x3)  # (bs, 32, T, F)\n",
    "        \n",
    "        x = self.after_conv2(x)\n",
    "        # (batch_size, subbands_num * target_sources_num * input_channles * self.K, T, F')\n",
    "        # # print(33, \"x.shape\", x.shape)\n",
    "\n",
    "        # Recover shape\n",
    "        x = F.pad(x, pad=(0, 1))  # Pad frequency, e.g., 256 -> 257.\n",
    "\n",
    "        x = x[:, :, 0:origin_len, :]\n",
    "        \n",
    "        # print(55, x.shape)\n",
    "        \n",
    "        audio_length = subband_x.shape[2]\n",
    "        \n",
    "        # Recover each subband spectrograms to subband waveforms. Then synthesis\n",
    "        # the subband waveforms to a waveform.\n",
    "        C1 = x.shape[1] // self.subbands_num\n",
    "        C2 = mag.shape[1] // self.subbands_num\n",
    "\n",
    "        separated_subband_audio = torch.cat(\n",
    "            [\n",
    "                self.feature_maps_to_wav(\n",
    "                    input_tensor=x[:, j * C1 : (j + 1) * C1, :, :],\n",
    "                    sp=mag[:, j * C2 : (j + 1) * C2, :, :],\n",
    "                    sin_in=sin_in[:, j * C2 : (j + 1) * C2, :, :],\n",
    "                    cos_in=cos_in[:, j * C2 : (j + 1) * C2, :, :],\n",
    "                    audio_length=audio_length,\n",
    "                )\n",
    "                for j in range(self.subbands_num)\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        separated_subband_audio = torch.unsqueeze(separated_subband_audio, 2)\n",
    "        # print(66, separated_subband_audio.shape)\n",
    "        \n",
    "        y = self.out_conv_block(separated_subband_audio)\n",
    "        \n",
    "        y = torch.squeeze(y, 2)\n",
    "        y = torch.squeeze(y, 2)\n",
    "                \n",
    "        \n",
    "#         # UNet\n",
    "#         # print(\"x\", x.shape)\n",
    "#         (x1_pool, x1) = self.encoder_block1(x)  # x1_pool: (bs, 32, T / 2, F / 2)\n",
    "#         # print(x1_pool.shape, x1.shape)\n",
    "#         (x2_pool, x2) = self.encoder_block2(x1_pool)  # x2_pool: (bs, 64, T / 4, F / 4)\n",
    "#         # print(x2_pool.shape, x2.shape)\n",
    "#         (x3_pool, x3) = self.encoder_block3(x2_pool)  # x3_pool: (bs, 128, T / 8, F / 8)\n",
    "#         # print(x3_pool.shape, x3.shape)\n",
    "#         # (x4_pool, x4) = self.encoder_block4(x3_pool)  # x4_pool: (bs, 256, T / 16, F / 16)\n",
    "#         # (x5_pool, x5) = self.encoder_block5(x4_pool)  # x5_pool: (bs, 384, T / 32, F / 32)\n",
    "#         # (x6_pool, x6) = self.encoder_block6(x5_pool)  # x6_pool: (bs, 384, T / 32, F / 64)\n",
    "#         (x_center, _) = self.conv_block7a(x3_pool)  # (bs, 384, T / 32, F / 64)\n",
    "#         (x_center, _) = self.conv_block7b(x_center)  # (bs, 384, T / 32, F / 64)\n",
    "#         # (x_center, _) = self.conv_block7c(x_center)  # (bs, 384, T / 32, F / 64)\n",
    "#         # (x_center, _) = self.conv_block7d(x_center)  # (bs, 384, T / 32, F / 64)\n",
    "#         # x7 = self.decoder_block1(x_center, x6)  # (bs, 384, T / 32, F / 32)\n",
    "#         # x8 = self.decoder_block2(x7, x5)  # (bs, 384, T / 16, F / 16)\n",
    "#         # x9 = self.decoder_block3(x8, x4)  # (bs, 256, T / 8, F / 8)\n",
    "#         # print(\"x_center.shape, x3.shape\", x_center.shape, x3.shape)\n",
    "#         x10 = self.decoder_block4(x_center, x3)  # (bs, 128, T / 4, F / 4)\n",
    "#         x11 = self.decoder_block5(x10, x2)  # (bs, 64, T / 2, F / 2)\n",
    "#         x12 = self.decoder_block6(x11, x1)  # (bs, 32, T, F)\n",
    "        \n",
    "#         (x, _) = self.after_conv_block1(x12)  # (bs, 32, T, F)\n",
    "        \n",
    "#         x = self.after_conv2(x)\n",
    "#         # (batch_size, subbands_num * target_sources_num * input_channles * self.K, T, F')\n",
    "#         # print(33, \"x.shape\", x.shape)\n",
    "\n",
    "#         # Recover shape\n",
    "#         x = F.pad(x, pad=(0, 1))  # Pad frequency, e.g., 256 -> 257.\n",
    "\n",
    "#         x = x[:, :, 0:origin_len, :]\n",
    "#         # (batch_size, subbands_num * target_sources_num * input_channles * self.K, T, F')\n",
    "\n",
    "#         audio_length = subband_x.shape[2]\n",
    "        \n",
    "#         # Recover each subband spectrograms to subband waveforms. Then synthesis\n",
    "#         # the subband waveforms to a waveform.\n",
    "#         C1 = x.shape[1] // self.subbands_num\n",
    "#         C2 = mag.shape[1] // self.subbands_num\n",
    "\n",
    "#         separated_subband_audio = torch.cat(\n",
    "#             [\n",
    "#                 self.feature_maps_to_wav(\n",
    "#                     input_tensor=x[:, j * C1 : (j + 1) * C1, :, :],\n",
    "#                     sp=mag[:, j * C2 : (j + 1) * C2, :, :],\n",
    "#                     sin_in=sin_in[:, j * C2 : (j + 1) * C2, :, :],\n",
    "#                     cos_in=cos_in[:, j * C2 : (j + 1) * C2, :, :],\n",
    "#                     audio_length=audio_length,\n",
    "#                 )\n",
    "#                 for j in range(self.subbands_num)\n",
    "#             ],\n",
    "#             dim=1,\n",
    "#         )\n",
    "#         # （batch_size, subbands_num * target_sources_num * input_channles, segment_samples)\n",
    "        \n",
    "#         separated_subband_audio = torch.unsqueeze(separated_subband_audio, 2)\n",
    "#         (y, _) = self.out_conv_block(separated_subband_audio)\n",
    "        \n",
    "#         y = torch.squeeze(y, 2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "        \n",
    "        \n",
    "tmp_x = torch.rand(3, 9, 250)\n",
    "# \n",
    "# input_dict = {\n",
    "#     \"waveform\": tmp_x\n",
    "# }\n",
    "\n",
    "tmp_layer = MyModel(input_channels=9, target_sources_num=10)\n",
    "tmp_y = tmp_layer(tmp_x)\n",
    "tmp_y.shape\n",
    "\n",
    "# 99 torch.Size([3, 360, 41, 33])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# train_acc = torchmetrics.Accuracy()\n",
    "\n",
    "# model = nn.Linear(100, 1) # predict logits for 5 classes\n",
    "# x = torch.randn(1, 10, 100)\n",
    "# y = torch.randint(0, 2, (1, 10, 1)).double()\n",
    "# print(y.shape, y)\n",
    "\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "# for epoch in range(20):\n",
    "#     optimizer.zero_grad()\n",
    "#     output = model(x)\n",
    "#     print(\"output.shape, y.shape\", output.shape, y.shape)\n",
    "#     loss = criterion(output, y)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     acc = train_acc(output, y.long())\n",
    "#     print('Loss: {:.3f}, Acc: {:.3f} '.format(loss.item(), acc.item()))\n",
    "\n",
    "\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# train_acc = torchmetrics.Accuracy()\n",
    "\n",
    "# model = nn.Conv1d(10, 10, kernel_size=1000, groups=10)\n",
    "# x = torch.randn(3, 10, 1000)\n",
    "# y = torch.randint(0, 3, (3,))\n",
    "# print(y.shape, y)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "# for epoch in range(20):\n",
    "#     optimizer.zero_grad()\n",
    "#     output = model(x)\n",
    "#     output = torch.squeeze(output)\n",
    "#     print(\"output.shape, y.shape\", output.shape, y.shape)\n",
    "#     loss = criterion(output, y)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     acc = train_acc(output, y.long())\n",
    "#     print('Loss: {:.3f}, Acc: {:.3f} '.format(loss.item(), acc.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "from splearn.nn.base import LightningModel\n",
    "\n",
    "\n",
    "class LightningModelClassifier(LightningModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer=\"adamw\",\n",
    "        scheduler=\"cosine_with_warmup\",\n",
    "        optimizer_learning_rate: float=1e-3,\n",
    "        optimizer_epsilon: float=1e-6,\n",
    "        optimizer_weight_decay: float=0.0005,\n",
    "        scheduler_warmup_epochs: int=10,\n",
    "        criterion=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.train_acc = torchmetrics.Accuracy()\n",
    "        self.valid_acc = torchmetrics.Accuracy()\n",
    "        self.test_acc = torchmetrics.Accuracy()\n",
    "        \n",
    "        self.criterion_classifier = criterion\n",
    "        if self.criterion_classifier is None:\n",
    "            self.criterion_classifier = nn.CrossEntropyLoss()\n",
    "                \n",
    "    def build_model(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_hat = self.model(x)\n",
    "        return y_hat\n",
    "    \n",
    "    def step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.criterion_classifier(y_hat, y)\n",
    "        return y_hat, y, loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_hat, y, loss = self.step(batch, batch_idx)\n",
    "        acc = self.train_acc(y_hat, y)\n",
    "        self.log('train_loss', loss, on_step=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_hat, y, loss = self.step(batch, batch_idx)\n",
    "        acc = self.valid_acc(y_hat, y)\n",
    "        self.log('valid_loss', loss, on_step=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        y_hat, y, loss = self.step(batch, batch_idx)\n",
    "        acc = self.test_acc(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def training_epoch_end(self, outs):\n",
    "        self.log('train_acc_epoch', self.train_acc.compute())\n",
    "        \n",
    "    def validation_epoch_end(self, outs):\n",
    "        self.log('valid_acc_epoch', self.valid_acc.compute())\n",
    "    \n",
    "    def test_epoch_end(self, outs):\n",
    "        self.log('test_acc_epoch', self.test_acc.compute())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from splearn.nn.base import LightningModelClassifier\n",
    "\n",
    "\n",
    "class MultilabelLClassifier(LightningModelClassifier):\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer=\"adamw\",\n",
    "        scheduler=\"cosine_with_warmup\",\n",
    "        optimizer_learning_rate: float=1e-3,\n",
    "        optimizer_epsilon: float=1e-6,\n",
    "        optimizer_weight_decay: float=0.0005,\n",
    "        scheduler_warmup_epochs: int=10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.criterion_classifier = nn.CrossEntropyLoss() # nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def build_model(self, model, model_output_dim, num_classes, **kwargs):\n",
    "        self.model = model\n",
    "        # self.classifier = nn.Linear(model_output_dim*num_classes, num_classes)\n",
    "        # self.classifier = nn.Conv1d(num_classes, num_classes, kernel_size=model_output_dim, groups=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        # x = torch.flatten(x, 1)\n",
    "        # y_hat = self.classifier(x)\n",
    "        # y_hat = torch.squeeze(y_hat, 2)\n",
    "        return x\n",
    "    \n",
    "    def train_val_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        # y = torch.unsqueeze(y, 2).double()\n",
    "        y_hat = self.forward(x)\n",
    "        # y_hat = torch.sigmoid(y_hat)\n",
    "        loss = self.criterion_classifier(y_hat, y.long())\n",
    "        # loss = F.cross_entropy(y_hat, y)\n",
    "        return y_hat, y, loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_hat, y, loss = self.train_val_step(batch, batch_idx)\n",
    "        acc = self.train_acc(y_hat, y.long())\n",
    "        self.log('train_loss', loss, on_step=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_hat, y, loss = self.train_val_step(batch, batch_idx)\n",
    "        acc = self.valid_acc(y_hat, y.long())\n",
    "        self.log('valid_loss', loss, on_step=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        y_hat, y, loss = self.train_val_step(batch, batch_idx)\n",
    "        acc = self.test_acc(y_hat, y.long())\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "# x = torch.rand(3, 9, config.data.sample_length)\n",
    "# y = torch.randint(0, 2, (3,))\n",
    "\n",
    "# unet = ResUNet143_Subbandtime(input_channels=config.data.input_channels, target_sources_num=config.data.target_sources_num)\n",
    "# model = MultilabelLClassifier(\n",
    "#     optimizer=config.model.optimizer,\n",
    "#     scheduler=config.model.scheduler,\n",
    "#     optimizer_learning_rate=config.training.learning_rate,\n",
    "#     scheduler_warmup_epochs=config.training.num_warmup_epochs,\n",
    "# )\n",
    "# model.build_model(model=unet, model_output_dim=config.data.sample_length, num_classes=config.data.num_classes)\n",
    "\n",
    "# tmp_y = model(x)\n",
    "# print(\"tmp_y\", tmp_y.shape)\n",
    "# print(tmp_y)\n",
    "\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "# for epoch in range(10):\n",
    "#     optimizer.zero_grad()\n",
    "#     output = model(x)\n",
    "#     loss = criterion(output, y)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     print('Loss: {:.3f}'.format(loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # x = torch.rand(3, 9, 1000)\n",
    "# # y = torch.randint(0, 2, (3,)).double()\n",
    "\n",
    "# # unet = ResUNet143_Subbandtime(input_channels=config.data.input_channels, target_sources_num=config.data.target_sources_num)\n",
    "# # model = MultilabelLClassifier(\n",
    "# #     optimizer=config.model.optimizer,\n",
    "# #     scheduler=config.model.scheduler,\n",
    "# #     optimizer_learning_rate=config.training.learning_rate,\n",
    "# #     scheduler_warmup_epochs=config.training.num_warmup_epochs,\n",
    "# # )\n",
    "# # model.build_model(model=unet, model_output_dim=config.data.sample_length)\n",
    "\n",
    "# model = nn.Linear(32,5)\n",
    "# x = torch.rand(3, 32)\n",
    "# y = torch.randint(0, 2, (3,))\n",
    "# print(y)\n",
    "\n",
    "# tmp_y = model(x)\n",
    "# print(\"tmp_y\", tmp_y.shape)\n",
    "# print(tmp_y)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss() # torch.nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "# for epoch in range(20):\n",
    "#     optimizer.zero_grad()\n",
    "#     output = model(x)\n",
    "#     loss = criterion(output, y)\n",
    "#     # loss = F.cross_entropy(output, y)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     print('Loss: {:.3f}'.format(loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Global seed set to 1234\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc_epoch': 0.1875, 'test_loss': 16.77286720275879}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1875"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_subject_id = 33\n",
    "kfold_k = 0\n",
    "\n",
    "## init data\n",
    "train_dataset, val_dataset, test_dataset = data.get_train_val_test_dataset(test_subject_id=test_subject_id, kfold_k=kfold_k)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.training.batchsize, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.training.batchsize, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.training.batchsize, shuffle=False)\n",
    "\n",
    "## init model\n",
    "unet = MyModel(input_channels=config.data.input_channels, target_sources_num=config.data.target_sources_num)\n",
    "model = MultilabelLClassifier(\n",
    "    optimizer=config.model.optimizer,\n",
    "    scheduler=config.model.scheduler,\n",
    "    optimizer_learning_rate=config.training.learning_rate,\n",
    "    scheduler_warmup_epochs=config.training.num_warmup_epochs,\n",
    ")\n",
    "model.build_model(model=unet, model_output_dim=config.data.sample_length, num_classes=config.data.num_classes)\n",
    "\n",
    "## init training\n",
    "sub_dir = \"sub\"+ str(test_subject_id) +\"_k\"+ str(kfold_k)\n",
    "logger_tb = TensorBoardLogger(save_dir=\"tensorboard_logs\", name=config.run_name, sub_dir=sub_dir)\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "trainer = Trainer(max_epochs=config.training.num_epochs, gpus=config.training.gpus, logger=logger_tb, progress_bar_refresh_rate=0, weights_summary=None, callbacks=[lr_monitor])\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "## test\n",
    "\n",
    "result = trainer.test(dataloaders=test_loader, verbose=True)\n",
    "test_acc = result[0]['test_acc_epoch']\n",
    "test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1875"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_subject_id = 33\n",
    "# kfold_k = 0\n",
    "\n",
    "# train_dataset, val_dataset, test_dataset = data.get_train_val_test_dataset(test_subject_id=test_subject_id, kfold_k=kfold_k)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=config.training.batchsize, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=config.training.batchsize, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=config.training.batchsize, shuffle=False)\n",
    "\n",
    "# print(\"train_loader\", train_loader.dataset.data.shape, train_loader.dataset.targets.shape)\n",
    "# print(\"val_loader\", val_loader.dataset.data.shape, val_loader.dataset.targets.shape)\n",
    "# print(\"test_loader\", test_loader.dataset.data.shape, test_loader.dataset.targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_acc = torchmetrics.Accuracy()\n",
    "\n",
    "# # index = 0:2\n",
    "\n",
    "# x = torch.tensor(train_loader.dataset.data[0:10])\n",
    "# y = torch.tensor(train_loader.dataset.targets[0:10])\n",
    "# # x = torch.unsqueeze(x, 0)\n",
    "# # y = torch.unsqueeze(y, 0)\n",
    "# # y = torch.unsqueeze(y, 2)\n",
    "# print(x.shape, y.shape)\n",
    "# y_hat = model(x)\n",
    "# y_hat = torch.sigmoid(y_hat)\n",
    "\n",
    "# acc = tmp_acc(y_hat, y.long())\n",
    "# print(\"acc\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # trial = pred_y[0]\n",
    "# # for i in trial:\n",
    "# #     print(i)\n",
    "\n",
    "# N = 2\n",
    "# C = 40\n",
    "\n",
    "# outputs = torch.squeeze(y_hat)\n",
    "# labels = torch.squeeze(y)\n",
    "\n",
    "# outputs = torch.sigmoid(outputs)  # torch.Size([N, C]) e.g. tensor([[0., 0.5, 0.]])\n",
    "# outputs[outputs < 0.5] = 0\n",
    "# outputs[outputs >= 0.5] = 1\n",
    "# accuracy = (outputs == labels).sum()/(N*C)*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
